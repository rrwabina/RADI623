{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Abstractive text summarization"
      ],
      "metadata": {
        "id": "gsypoXkGeY9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "H0BTnlKTecDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3CTPT0JecuM",
        "outputId": "6de6ed0f-d6b3-4454-eed8-2cc86a23e01e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_QKlCqRgH9V",
        "outputId": "74e05011-887e-4c97-bfc9-0adbd6e4f04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.98)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data\n",
        "We going to used this data for all text summarization models"
      ],
      "metadata": {
        "id": "F6AREx3DjaeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text to summarize\n",
        "original_text = '''\n",
        "This past week, two of Thailand's largest cities, Bangkok and Chiang Mai,\n",
        "earned the ignominious privilege of being among the 10 cities of the world\n",
        "with the worst air quality during that period. The Ministry of Public\n",
        "Health has blamed air pollution for causing 200,000 hospital admissions\n",
        "in the past week alone.\n",
        "Air pollution is one of Thailand's largest killers, more than obesity,\n",
        "smoking, and even Covid-19. It accounted for over 50,000 premature\n",
        "deaths in 2021, reducing average life expectancy by two years. Further,\n",
        "there is widespread public concern that air pollution will reduce one of the\n",
        "country's main sources of income, tourism, in places like Chiang Mai.\n",
        "However, despite these grim statistics, this year's air pollution menace and\n",
        "the government's ham-fisted response seems like deja vu. During the first\n",
        "few months of every year, the level of air pollution spikes to hazardous\n",
        "levels and smog covers the skies.\n",
        "Every year the government responds by proclaiming a ban on forest fires\n",
        "(but inadequately enforcing this ban), asking people to wear masks and\n",
        "stay indoors, spraying water (which doesn't do much), unhelpfully and\n",
        "incorrectly blaming smallholder farmers, and expressing grave concern\n",
        "about the problem.\n",
        "But every year the government fails to address the underlying drivers of\n",
        "the problem. Prime Minister Prayut Chan-o-cha has halted three draft\n",
        "laws related to air pollution.\n",
        "There was hope the recently-elected Bangkok governor Chadchart\n",
        "Sittipunt would do more to address air pollution at least within Bangkok's\n",
        "jurisdiction. Yet his actions so far have been limited.\n",
        "The government's actions, however, are not surprising if you look at all of\n",
        "the major parties' platforms for the upcoming election. None has\n",
        "prioritised air pollution, called for wide-ranging reforms, or made air\n",
        "pollution a major part of their campaign.\n",
        "The Thailand Development Research Institute says that of their 87 major\n",
        "policy promises, only three are environmentally-related. While more data\n",
        "on the sources of pollution would be helpful, it is clear that what the\n",
        "current government has been doing since 2014 (not to mention the actions\n",
        "of previous governments) has not worked.\n",
        "We know there are three major sources of air pollution in the country:\n",
        "transport, industry, and agriculture, and that pollution is worse in winter\n",
        "months when there is an upsurge in agricultural burning and a\n",
        "temperature inversion resulting in less wind and rain to disperse\n",
        "pollutants.\n",
        "How much any of these three sources contributes to the total amount\n",
        "varies from month to month and by location. For example, transport and\n",
        "industry emissions comprise a much larger share of total emissions in\n",
        "Bangkok than in Chiang Mai, where the vast majority (up to 90%) of\n",
        "emissions come from agriculture.\n",
        "Countries who have been able to reduce air pollution show us that while\n",
        "this is a wickedly difficult problem to solve, it is not impossible and there\n",
        "are policy solutions out there which could reduce pollutant levels and\n",
        "improve health nationwide. So, let's look at each of three sources.\n",
        "1. Transport: In Bangkok, vehicular emissions are high due to the presence\n",
        "of many older, high-polluting vehicles, together with a drastic increase in\n",
        "the number of cars in recent years. To reverse these trends, the\n",
        "government could initiate something like the USA's \"cash for clunkers\"\n",
        "programme, which provides incentives for citizens to replace older, more\n",
        "polluting cars with newer, cleaner, and more fuel-efficient ones.\n",
        "A number of cities not only have designated bus lanes but also switched\n",
        "their fleets to new vehicles powered by electricity or natural gas. Both\n",
        "policies could incentivise the public to use public buses more. Finally,\n",
        "cities like Singapore and London were able to significantly reduce their air\n",
        "pollution and traffic congestion by introducing congestion pricing\n",
        "schemes.\n",
        "2. Industry: Thailand has no emissions inventory database to record\n",
        "industrial emissions, despite having around 140,000 polluting factories. A\n",
        "head of a local NGO told me: \"Since there are no emissions inventories\n",
        "from factories, we're working blind.\" Further, in 2019, the National\n",
        "Legislative Assembly revised the Factory Act 1992 so that only industrial\n",
        "companies with more than 50 employees and machinery exceeding 50\n",
        "horsepower are subject to monitoring for waste discharge and anti-\n",
        "pollution measures, including air pollution.\n",
        "Additionally, the authority to fine major polluters rests with the\n",
        "Department of Industrial Works (DIW) under the Ministry of Industry but\n",
        "this creates a conflict of interest since DIW's mandate is to expand\n",
        "industrial growth without any curbs. Thailand needs a law that requires\n",
        "polluting factories to disclose their emissions, such as United States'\n",
        "Toxics Release Inventory and the European Pollutant Release and\n",
        "Transfer Register. This new law would make factory permits for operation\n",
        "dependent upon lowering their emissions.\n",
        "3. Agriculture: While most hotspots of biomass burning that cause\n",
        "pollution inside Thailand are in fact outside its boundaries, a large\n",
        "percentage still occurs within Thailand, particularly stemming from maize,\n",
        "sugarcane, and rice harvesting. Thai agribusinesses have a high degree of\n",
        "culpability for burning in neighbouring countries, such as Laos and\n",
        "Myanmar, due to their investments and introduction of contract farming\n",
        "schemes there.\n",
        "However, no information has been released on which companies are\n",
        "responsible for the burning and no government has ever held these\n",
        "agribusinesses accountable or penalised them for the burning. A good\n",
        "example Thailand could follow is Singapore's 2014 Transboundary Haze\n",
        "Pollution Act that targets the business sector by imposing fines on\n",
        "companies with operations in neighbouring countries found to contribute\n",
        "to haze pollution within Singapore's borders.\n",
        "Moreover, the government could insist upon stringent product standards,\n",
        "such as no burnt sugarcane, and could help farmers by subsiding the\n",
        "purchase of harvesting machines and introducing other cleaner\n",
        "production methods.\n",
        "Overall, while legislation can never be a single silver bullet solution, no\n",
        "country that has achieved cleaner air quality has done so without first\n",
        "having sensible air pollution policies in place. For example, the USA, UK,\n",
        "and Singapore have all passed Clean Air Acts.\n",
        "The citizen-driven proposed \"Thai Clean Air Act\" Act provides the tools to\n",
        "address the underlying causes that have so far impeded the resolution of\n",
        "this public health crisis.\n",
        "The bill adopts a rights-based approach that establishes the public's right\n",
        "to clean air and by doing so concurrently creates an obligation of the state\n",
        "to protect this right.\n",
        "Finally, it includes economic incentives to push current major polluters to\n",
        "reduce their emissions.\n",
        "We hope that all parties will show that they truly care about the health and\n",
        "lives of the people and will seek to adopt these policies and enact the\n",
        "citizen-led Thai Clean Air Act.\n",
        "Other countries have successfully improved their air quality, so why not\n",
        "Thailand too?\n",
        "Danny Marks is an Assistant Professor of Environmental Politics and\n",
        "Policy at Dublin City University. Weenarin Lulitanonda is a co-founder of\n",
        "the Thailand Clean Air Network.\n",
        "'''"
      ],
      "metadata": {
        "id": "nngUZVFZjXAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarization with T5\n",
        "T5 is an encoder-decoder model. It converts all language problems into a text-to-text format."
      ],
      "metadata": {
        "id": "fbWpp8aPe5l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing requirements\n",
        "  # T5ForConditionalGeneration for both input and output are sequences.\n",
        "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "uneia4fHekNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import model"
      ],
      "metadata": {
        "id": "1iSPGXFlgytM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the model and tokenizer \n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Vh4kfffdI9",
        "outputId": "7948d14a-282b-4ee3-abe5-d5a193e3e051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is the most important step which you should not forget. You have to add the string ” summarize: ” at the beginning of your raw text . T5 transformers performs different tasks by prepending the particular prefix to the input text."
      ],
      "metadata": {
        "id": "52LvdwK6gVwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating the word \"summarize:\" to raw text\n",
        "text = \"summarize:\" + original_text\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "hw6Qf8AwfyNM",
        "outputId": "3ec1cc5c-1a54-4da3-e1a3-59f7a2402f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'summarize:\\nThis past week, two of Thailand\\'s largest cities, Bangkok and Chiang Mai,\\nearned the ignominious privilege of being among the 10 cities of the world\\nwith the worst air quality during that period. The Ministry of Public\\nHealth has blamed air pollution for causing 200,000 hospital admissions\\nin the past week alone.\\nAir pollution is one of Thailand\\'s largest killers, more than obesity,\\nsmoking, and even Covid-19. It accounted for over 50,000 premature\\ndeaths in 2021, reducing average life expectancy by two years. Further,\\nthere is widespread public concern that air pollution will reduce one of the\\ncountry\\'s main sources of income, tourism, in places like Chiang Mai.\\nHowever, despite these grim statistics, this year\\'s air pollution menace and\\nthe government\\'s ham-fisted response seems like deja vu. During the first\\nfew months of every year, the level of air pollution spikes to hazardous\\nlevels and smog covers the skies.\\nEvery year the government responds by proclaiming a ban on forest fires\\n(but inadequately enforcing this ban), asking people to wear masks and\\nstay indoors, spraying water (which doesn\\'t do much), unhelpfully and\\nincorrectly blaming smallholder farmers, and expressing grave concern\\nabout the problem.\\nBut every year the government fails to address the underlying drivers of\\nthe problem. Prime Minister Prayut Chan-o-cha has halted three draft\\nlaws related to air pollution.\\nThere was hope the recently-elected Bangkok governor Chadchart\\nSittipunt would do more to address air pollution at least within Bangkok\\'s\\njurisdiction. Yet his actions so far have been limited.\\nThe government\\'s actions, however, are not surprising if you look at all of\\nthe major parties\\' platforms for the upcoming election. None has\\nprioritised air pollution, called for wide-ranging reforms, or made air\\npollution a major part of their campaign.\\nThe Thailand Development Research Institute says that of their 87 major\\npolicy promises, only three are environmentally-related. While more data\\non the sources of pollution would be helpful, it is clear that what the\\ncurrent government has been doing since 2014 (not to mention the actions\\nof previous governments) has not worked.\\nWe know there are three major sources of air pollution in the country:\\ntransport, industry, and agriculture, and that pollution is worse in winter\\nmonths when there is an upsurge in agricultural burning and a\\ntemperature inversion resulting in less wind and rain to disperse\\npollutants.\\nHow much any of these three sources contributes to the total amount\\nvaries from month to month and by location. For example, transport and\\nindustry emissions comprise a much larger share of total emissions in\\nBangkok than in Chiang Mai, where the vast majority (up to 90%) of\\nemissions come from agriculture.\\nCountries who have been able to reduce air pollution show us that while\\nthis is a wickedly difficult problem to solve, it is not impossible and there\\nare policy solutions out there which could reduce pollutant levels and\\nimprove health nationwide. So, let\\'s look at each of three sources.\\n1. Transport: In Bangkok, vehicular emissions are high due to the presence\\nof many older, high-polluting vehicles, together with a drastic increase in\\nthe number of cars in recent years. To reverse these trends, the\\ngovernment could initiate something like the USA\\'s \"cash for clunkers\"\\nprogramme, which provides incentives for citizens to replace older, more\\npolluting cars with newer, cleaner, and more fuel-efficient ones.\\nA number of cities not only have designated bus lanes but also switched\\ntheir fleets to new vehicles powered by electricity or natural gas. Both\\npolicies could incentivise the public to use public buses more. Finally,\\ncities like Singapore and London were able to significantly reduce their air\\npollution and traffic congestion by introducing congestion pricing\\nschemes.\\n2. Industry: Thailand has no emissions inventory database to record\\nindustrial emissions, despite having around 140,000 polluting factories. A\\nhead of a local NGO told me: \"Since there are no emissions inventories\\nfrom factories, we\\'re working blind.\" Further, in 2019, the National\\nLegislative Assembly revised the Factory Act 1992 so that only industrial\\ncompanies with more than 50 employees and machinery exceeding 50\\nhorsepower are subject to monitoring for waste discharge and anti-\\npollution measures, including air pollution.\\nAdditionally, the authority to fine major polluters rests with the\\nDepartment of Industrial Works (DIW) under the Ministry of Industry but\\nthis creates a conflict of interest since DIW\\'s mandate is to expand\\nindustrial growth without any curbs. Thailand needs a law that requires\\npolluting factories to disclose their emissions, such as United States\\'\\nToxics Release Inventory and the European Pollutant Release and\\nTransfer Register. This new law would make factory permits for operation\\ndependent upon lowering their emissions.\\n3. Agriculture: While most hotspots of biomass burning that cause\\npollution inside Thailand are in fact outside its boundaries, a large\\npercentage still occurs within Thailand, particularly stemming from maize,\\nsugarcane, and rice harvesting. Thai agribusinesses have a high degree of\\nculpability for burning in neighbouring countries, such as Laos and\\nMyanmar, due to their investments and introduction of contract farming\\nschemes there.\\nHowever, no information has been released on which companies are\\nresponsible for the burning and no government has ever held these\\nagribusinesses accountable or penalised them for the burning. A good\\nexample Thailand could follow is Singapore\\'s 2014 Transboundary Haze\\nPollution Act that targets the business sector by imposing fines on\\ncompanies with operations in neighbouring countries found to contribute\\nto haze pollution within Singapore\\'s borders.\\nMoreover, the government could insist upon stringent product standards,\\nsuch as no burnt sugarcane, and could help farmers by subsiding the\\npurchase of harvesting machines and introducing other cleaner\\nproduction methods.\\nOverall, while legislation can never be a single silver bullet solution, no\\ncountry that has achieved cleaner air quality has done so without first\\nhaving sensible air pollution policies in place. For example, the USA, UK,\\nand Singapore have all passed Clean Air Acts.\\nThe citizen-driven proposed \"Thai Clean Air Act\" Act provides the tools to\\naddress the underlying causes that have so far impeded the resolution of\\nthis public health crisis.\\nThe bill adopts a rights-based approach that establishes the public\\'s right\\nto clean air and by doing so concurrently creates an obligation of the state\\nto protect this right.\\nFinally, it includes economic incentives to push current major polluters to\\nreduce their emissions.\\nWe hope that all parties will show that they truly care about the health and\\nlives of the people and will seek to adopt these policies and enact the\\ncitizen-led Thai Clean Air Act.\\nOther countries have successfully improved their air quality, so why not\\nThailand too?\\nDanny Marks is an Assistant Professor of Environmental Politics and\\nPolicy at Dublin City University. Weenarin Lulitanonda is a co-founder of\\nthe Thailand Clean Air Network.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data preparation"
      ],
      "metadata": {
        "id": "hbCErN9Dg3NP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you recall , T5 is a encoder-decoder mode and hence the input sequence should be in the form of a sequence of ids, or input-ids.\n",
        "\n",
        "convert the input text into input-ids by encode() method"
      ],
      "metadata": {
        "id": "eslVVwSphIdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the input text\n",
        "input_ids = t5_tokenizer.encode(text, return_tensors='pt', max_length=512)\n",
        "input_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4zfuVFPgZO4",
        "outputId": "292fe9de-5699-439d-ab9b-463c200f55e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pass input_ids to the function generate(), which will return a sequence of ids corresponding to the summary."
      ],
      "metadata": {
        "id": "O2GtkySwhokc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate summary"
      ],
      "metadata": {
        "id": "RIxB791siwBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating summary ids\n",
        "summary_ids = t5_model.generate(input_ids, max_length=100, min_length=30)\n",
        "print(summary_ids.shape)\n",
        "print(summary_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmk1oPzsglCk",
        "outputId": "3c2c46c8-ae4b-4e59-c5a0-2617f94bfda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 59])\n",
            "tensor([[    0,   799, 10441,    19,    80,    13, 10508,    31,     7,  2015,\n",
            "         14804,     7,     6,    72,   145, 18719,     6, 10257,     6,    11,\n",
            "           237,   638,  6961,  4481,     3,     5,     3,  3565, 20425,  7475,\n",
            "             6,    48,   215,    31,     7,   799, 10441, 24034,  1330,   114,\n",
            "          6009,  9056,     3,     5,   334,   215,     8,   789, 13288,    12,\n",
            "          1115,     8,  3863,    13,     8,   682,     3,     5,     1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that model has returned a tensor with sequence of ids. Now, use the decode() function to generate the summary text from these ids."
      ],
      "metadata": {
        "id": "7vi23On8h_mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding the tensor and printing the summary.\n",
        "t5_summary = t5_tokenizer.decode(summary_ids[0])\n",
        "t5_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "yEN8f0rpg9RQ",
        "outputId": "d7b0b92e-d98a-427e-b1e5-94cfd61de3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<pad> air pollution is one of Thailand's largest killers, more than obesity, smoking, and even Covid-19. despite grim statistics, this year's air pollution menace seems like deja vu. every year the government fails to address the drivers of the problem.</s>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization with BART\n",
        "BART is one of variant of BERT, aimed for the generation problem."
      ],
      "metadata": {
        "id": "wtcYNj9Ei36W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the model\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig"
      ],
      "metadata": {
        "id": "CCYCepUiim7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import model"
      ],
      "metadata": {
        "id": "fJu55x_ajz17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the model and tokenizer. For problems where there is need to generate sequences , it is preferred to use BartForConditionalGeneration model.\n",
        "\n",
        "” bart-large-cnn” is a pretrained model, fine tuned especially for summarization task."
      ],
      "metadata": {
        "id": "WIOpwJgLkCcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model and tokenizer for bart-large-cnn\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"
      ],
      "metadata": {
        "id": "MUU0iv1zjzW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data prepration"
      ],
      "metadata": {
        "id": "zLHJ7Zacksga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pass the input text in the form of a sequence of ids.\n",
        "\n",
        "For this, use the batch_encode_plus() function with the tokenizer. This function returns a dictionary containing the encoded sequence or sequence pair, etc.\n",
        "\n",
        "Set the max_length parameter in batch_encode_plus()."
      ],
      "metadata": {
        "id": "ikfbKUkskMEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the inputs\n",
        "input_ids = bart_tokenizer.batch_encode_plus([original_text], \n",
        "                                              max_length = 512,\n",
        "                                              return_tensors='pt')\n",
        "input_ids['input_ids'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz0WuACkj2ui",
        "outputId": "4670a3dc-496d-46b5-f0d2-9e88094016e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate summary"
      ],
      "metadata": {
        "id": "5dBOm0yzmWOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the inputs and passing them to model.generate()\n",
        "summary_ids = bart_model.generate(input_ids['input_ids'], \n",
        "                                  early_stopping=True, \n",
        "                                  max_length=100, \n",
        "                                  min_length=30)\n",
        "print(summary_ids.shape)\n",
        "print(summary_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE9n2Sj_kjUk",
        "outputId": "956773db-3f9a-44f4-ded9-a65397f9b8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 46])\n",
            "tensor([[    2,     0, 17906,  6631,    16,    65,     9,  6547,    18,  1154,\n",
            "         20480,     6,    55,    87, 14057,     6,  7893,     6,     8,   190,\n",
            "         19150,   808,    12,  1646,     4,  1489,   692,  2869,   857,  1182,\n",
            "          8710,    12,   139,    12,  7794,    34, 12856,   130,  2479,  2074,\n",
            "          1330,     7,   935,  6631,     4,     2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.generate() has returned a sequence of ids corresponding to the summary of original text. You can convert the sequence of ids to text through decode() method."
      ],
      "metadata": {
        "id": "TrcdcImUmF4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding and printing the summary\n",
        "bart_summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "bart_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MmFR50lrlgun",
        "outputId": "6f6e34b1-f8f7-4de4-d5d4-b5c4a3461b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Air pollution is one of Thailand's largest killers, more than obesity, smoking, and even Covid-19. Prime Minister Prayut Chan-o-cha has halted three draft laws related to air pollution.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization with GPT-2\n",
        "\n",
        "GPT-2 transformer is another major player in text summarization, introduced by OpenAI.\n",
        "<br>\n",
        "We can only use GPT-2, currently there is no GPT-3 released"
      ],
      "metadata": {
        "id": "HYk55ypyma3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, you have to import the tokenizer and model. Make sure that you import a LM Head type model, as it is necessary to generate sequences. Next, load the pretrained gpt-2 model and tokenize"
      ],
      "metadata": {
        "id": "wdHNSpdKne9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing model and tokenizer\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "PQm0taD9mJyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import model"
      ],
      "metadata": {
        "id": "MNoC7tiTna8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the model and tokenizer with gpt-2\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "vwENlEGmnm8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data prepration\n",
        "basically we use library from huggingface, it is easy as you could see, so we do almost the same thing from previous models"
      ],
      "metadata": {
        "id": "fVyGlnkYn1l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding text to get input ids & pass them to model.generate()\n",
        "input_ids = gpt_tokenizer.batch_encode_plus([original_text],return_tensors='pt',max_length=512)\n",
        "input_ids['input_ids'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyIRTV2XnrCU",
        "outputId": "a4d95c06-aa99-484f-a90b-b6fa9e39c187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate summary"
      ],
      "metadata": {
        "id": "3S9_3wykoTyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the summary_ids contains the sequence of ids corresponding to the text summary . You can decode it and print the summary"
      ],
      "metadata": {
        "id": "Q7ZpET-RoSCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pass them to model.generate()\n",
        "summary_ids = gpt_model.generate(input_ids['input_ids'], \n",
        "                                 max_length=100,\n",
        "                                 early_stopping=True)\n",
        "print(summary_ids.shape)\n",
        "print(summary_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiYOGgENoOoF",
        "outputId": "3c85ed85-b2da-4204-a627-cd38c9b9315f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Input length of input_ids is 512, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 513])\n",
            "tensor([[  198,  1212,  1613,  1285,    11,   734,   286, 16952,   338,  4387,\n",
            "          4736,    11, 35007,   290,   609, 15483, 36709,    11,   198, 39123,\n",
            "           262,  3627,  6351,   699, 11941,   286,   852,  1871,   262,   838,\n",
            "          4736,   286,   262,   995,   198,  4480,   262,  5290,  1633,  3081,\n",
            "          1141,   326,  2278,    13,   383,  9475,   286,  5094,   198, 18081,\n",
            "           468, 13772,  1633, 12231,   329,  6666,   939,    11,   830,  4436,\n",
            "         25349,   198,   259,   262,  1613,  1285,  3436,    13,   198, 16170,\n",
            "         12231,   318,   530,   286, 16952,   338,  4387, 25542,    11,   517,\n",
            "           621, 13825,    11,   198, 48783,    11,   290,   772, 39751,   312,\n",
            "            12,  1129,    13,   632, 17830,   329,   625,  2026,    11,   830,\n",
            "         19905,   198, 22595,    82,   287, 33448,    11,  8868,  2811,  1204,\n",
            "         29098,   416,   734,   812,    13,  7735,    11,   198,  8117,   318,\n",
            "         10095,  1171,  2328,   326,  1633, 12231,   481,  4646,   530,   286,\n",
            "           262,   198, 19315,   338,  1388,  4237,   286,  3739,    11, 19277,\n",
            "            11,   287,  4113,   588,   609, 15483, 36709,    13,   198,  4864,\n",
            "            11,  3805,   777, 18288,  7869,    11,   428,   614,   338,  1633,\n",
            "         12231, 36292,   290,   198,  1169,  1230,   338,  8891,    12,    69,\n",
            "          6347,  2882,  2331,   588,   390,  6592,   410,    84,    13,  5856,\n",
            "           262,   717,   198, 32146,  1933,   286,   790,   614,    11,   262,\n",
            "          1241,   286,  1633, 12231, 27198,   284, 26082,   198, 46170,   290,\n",
            "           895,   519,  8698,   262, 24091,    13,   198,  6109,   614,   262,\n",
            "          1230, 20067,   416, 46431,   257,  3958,   319,  8222, 12252,   198,\n",
            "             7,  4360, 17881,  1286, 26587,   428,  3958,   828,  4737,   661,\n",
            "           284,  5806, 20680,   290,   198, 31712, 31797,    11, 40170,  1660,\n",
            "           357,  4758,  1595,   470,   466,   881,   828,   555, 16794,  2759,\n",
            "           290,   198,  1939, 47315,   306, 24630,  1402, 13829,  9818,    11,\n",
            "           290, 16621, 12296,  2328,   198, 10755,   262,  1917,    13,   198,\n",
            "          1537,   790,   614,   262,  1230, 10143,   284,  2209,   262, 10238,\n",
            "          6643,   286,   198,  1169,  1917,    13,  5537,  4139,  1736,   323,\n",
            "           315, 18704,    12,    78,    12, 11693,   468, 27771,  1115,  4538,\n",
            "           198, 29317,  3519,   284,  1633, 12231,    13,   198,  1858,   373,\n",
            "          2911,   262,  2904,    12, 28604, 35007,  8153, 19800, 40926,   198,\n",
            "            50,   715,   541,  2797,   561,   466,   517,   284,  2209,  1633,\n",
            "         12231,   379,  1551,  1626, 35007,   338,   198,    73,   333,  9409,\n",
            "          2867,    13,  6430,   465,  4028,   523,  1290,   423,   587,  3614,\n",
            "            13,   198,   464,  1230,   338,  4028,    11,  2158,    11,   389,\n",
            "           407,  6452,   611,   345,   804,   379,   477,   286,   198,  1169,\n",
            "          1688,  4671,     6,  9554,   329,   262,  7865,  3071,    13,  6045,\n",
            "           468,   198,  3448,   273,   270,  1417,  1633, 12231,    11,  1444,\n",
            "           329,  3094,    12, 32319, 12506,    11,   393,   925,  1633,   198,\n",
            "         30393,  1009,   257,  1688,   636,   286,   511,  1923,    13,   198,\n",
            "           464, 16952,  7712,  4992,  5136,  1139,   326,   286,   511, 10083,\n",
            "          1688,   198, 30586, 10497,    11,   691,  1115,   389, 34132,    12,\n",
            "          5363,    13,  2893,   517,  1366,   198,   261,   262,  4237,   286,\n",
            "         12231,   561,   307,  7613,    11,   340,   318,  1598,   326,   644,\n",
            "           262,   198, 14421,  1230,   468,   587,  1804,  1201,  1946,   357,\n",
            "          1662,   284,  3068,   262,  4028,   198,  1659,  2180,  6905,     8,\n",
            "           468,   407,  3111,    13,   198,  1135,   760,   612,   389,  1115,\n",
            "          1688,  4237,   286,  1633, 12231,   287,   262,  1499,    25,   198,\n",
            "          7645,   634,    11,  2831,    11,   290, 14510,    11,   290,   326,\n",
            "         12231,   318,  4785,   287,  7374,   198, 41537,   618,   612,   318,\n",
            "           281,   510, 46737]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding and printing summary\n",
        "gpt_summary = gpt_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(gpt_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPhOMVbJocmZ",
        "outputId": "f24b9d54-110f-4b77-c1a9-5d7e560ea72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "This past week, two of Thailand's largest cities, Bangkok and Chiang Mai,\n",
            "earned the ignominious privilege of being among the 10 cities of the world\n",
            "with the worst air quality during that period. The Ministry of Public\n",
            "Health has blamed air pollution for causing 200,000 hospital admissions\n",
            "in the past week alone.\n",
            "Air pollution is one of Thailand's largest killers, more than obesity,\n",
            "smoking, and even Covid-19. It accounted for over 50,000 premature\n",
            "deaths in 2021, reducing average life expectancy by two years. Further,\n",
            "there is widespread public concern that air pollution will reduce one of the\n",
            "country's main sources of income, tourism, in places like Chiang Mai.\n",
            "However, despite these grim statistics, this year's air pollution menace and\n",
            "the government's ham-fisted response seems like deja vu. During the first\n",
            "few months of every year, the level of air pollution spikes to hazardous\n",
            "levels and smog covers the skies.\n",
            "Every year the government responds by proclaiming a ban on forest fires\n",
            "(but inadequately enforcing this ban), asking people to wear masks and\n",
            "stay indoors, spraying water (which doesn't do much), unhelpfully and\n",
            "incorrectly blaming smallholder farmers, and expressing grave concern\n",
            "about the problem.\n",
            "But every year the government fails to address the underlying drivers of\n",
            "the problem. Prime Minister Prayut Chan-o-cha has halted three draft\n",
            "laws related to air pollution.\n",
            "There was hope the recently-elected Bangkok governor Chadchart\n",
            "Sittipunt would do more to address air pollution at least within Bangkok's\n",
            "jurisdiction. Yet his actions so far have been limited.\n",
            "The government's actions, however, are not surprising if you look at all of\n",
            "the major parties' platforms for the upcoming election. None has\n",
            "prioritised air pollution, called for wide-ranging reforms, or made air\n",
            "pollution a major part of their campaign.\n",
            "The Thailand Development Research Institute says that of their 87 major\n",
            "policy promises, only three are environmentally-related. While more data\n",
            "on the sources of pollution would be helpful, it is clear that what the\n",
            "current government has been doing since 2014 (not to mention the actions\n",
            "of previous governments) has not worked.\n",
            "We know there are three major sources of air pollution in the country:\n",
            "transport, industry, and agriculture, and that pollution is worse in winter\n",
            "months when there is an upswing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFORLlWhzBYQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}