{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9DRX2CMW9Tu"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yPEo57BaW1Pa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWIl-hpAXBhQ"
      },
      "source": [
        "First import the news summary dataset to your workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "wUMe89mCXEkq",
        "outputId": "2d40c878-5c42-403b-a9ed-0ae6f8a97ee5"
      },
      "outputs": [],
      "source": [
        "summary = pd.read_csv('../data/news_summary.csv',  encoding = \"ISO-8859-1\")\n",
        "raw = pd.read_csv('../data/news_summary_more.csv', encoding = \"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnD4QTp3XNrB"
      },
      "source": [
        "Combine data from the two CSV files into a single `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Jg4bIgLvXKlF"
      },
      "outputs": [],
      "source": [
        "pre1 = raw.iloc[:, 0:2].copy()\n",
        "pre2 = summary.iloc[:, 0:6].copy()\n",
        "\n",
        "# To increase the intake of possible text values to build a reliable model\n",
        "pre2['text'] = pre2['author'].str.cat(pre2['date'].str.cat(pre2['read_more'].str.cat(pre2['text'].str.cat(pre2['ctext'], sep=' '), sep=' '), sep=' '), sep=' ')\n",
        "\n",
        "pre = pd.DataFrame()\n",
        "pre['text'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\n",
        "pre['summary'] = pd.concat([pre1['headlines'], pre2['headlines']], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szx3I4EQatXp"
      },
      "source": [
        "Let's get a better understanding of the data by printing the first two rows to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "flI7w5u1alqC",
        "outputId": "914dba93-5bb2-4030-8b4a-17676ba05123"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...   \n",
              "1  Kunal Shah's credit card bill payment platform...   \n",
              "\n",
              "                                             summary  \n",
              "0  upGrad learner switches to career in ML & Al w...  \n",
              "1  Delhi techie wins free food from Swiggy for on...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEyLh7fSay6X"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JbZudikAavGt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Remove non-alphabetic characters (Data Cleaning)\n",
        "def text_strip(column):\n",
        "\n",
        "    for row in column:\n",
        "        row = re.sub(\"(\\\\t)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\\\r)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\\\n)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove _ if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(__+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove - if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(--+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove ~ if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(~~+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove + if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(\\+\\++)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove . if it occurs more than one time consecutively\n",
        "        row = re.sub(\"(\\.\\.+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove the characters - <>()|&©ø\"',;?~*!\n",
        "        row = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove mailto:\n",
        "        row = re.sub(\"(mailto:)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove \\x9* in text\n",
        "        row = re.sub(r\"(\\\\x9\\d)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Replace INC nums to INC_NUM\n",
        "        row = re.sub(\"([iI][nN][cC]\\d+)\", \"INC_NUM\", str(row)).lower()\n",
        "\n",
        "        # Replace CM# and CHG# to CM_NUM\n",
        "        row = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(row)).lower()\n",
        "\n",
        "        # Remove punctuations at the end of a word\n",
        "        row = re.sub(\"(\\.\\s+)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\-\\s+)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\:\\s+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Replace any url to only the domain name\n",
        "        try:\n",
        "            url = re.search(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", str(row))\n",
        "            repl_url = url.group(3)\n",
        "            row = re.sub(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", repl_url, str(row))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Remove multiple spaces\n",
        "        row = re.sub(\"(\\s+)\", \" \", str(row)).lower()\n",
        "\n",
        "        # Remove the single character hanging between any two spaces\n",
        "        row = re.sub(\"(\\s+.\\s+)\", \" \", str(row)).lower()\n",
        "\n",
        "        yield row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWcG9jyua67p"
      },
      "source": [
        "Call the `text_strip()` function on both the text and summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uZBxfMOAaySn"
      },
      "outputs": [],
      "source": [
        "processed_text = text_strip(pre['text'])\n",
        "processed_summary = text_strip(pre['summary'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDAD0QtKa_hq"
      },
      "source": [
        "Load the data as batches using the `pipe()` method provided by spaCy. This ensures that all pieces of text and summaries possess the string data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1v3v7I9Wa9q3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from time import time\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser']) \n",
        "\n",
        "# Process text as batches and yield Doc objects in order\n",
        "text = [str(doc) for doc in nlp.pipe(processed_text, batch_size=5000)]\n",
        "\n",
        "summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(processed_summary, batch_size=5000)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71BON6xRbGdU"
      },
      "source": [
        "The `_START_` and `_END_` tokens denote the start and end of the summary, respectively. This will be used later to detect and remove empty summaries.\n",
        "\n",
        "Now let's print some of the data to understand how it’s been loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8bCEMOrbDqm"
      },
      "outputs": [],
      "source": [
        "text[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR635bExbb_M"
      },
      "source": [
        "Print the summary as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASRT9pshbZ_q"
      },
      "outputs": [],
      "source": [
        "summary[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X91tJxJ4bfBB"
      },
      "source": [
        "# Determine the maximum permissible sequence lengths  \n",
        "Next, store the `text` and `summary` lists in pandas objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v8kEFK_bdmo"
      },
      "outputs": [],
      "source": [
        "pre['cleaned_text'] = pd.Series(text)\n",
        "pre['cleaned_summary'] = pd.Series(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW1zKBA4bs-c"
      },
      "source": [
        "Plot a graph to determine the frequency ranges tied to the lengths of text and summary, i.e., determine the range of length of words where the maximum number of texts and summaries fall into"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ElcR7bpbqd-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_count = []\n",
        "summary_count = []\n",
        "\n",
        "for sent in pre['cleaned_text']:\n",
        "    text_count.append(len(sent.split()))\n",
        "    \n",
        "for sent in pre['cleaned_summary']:\n",
        "    summary_count.append(len(sent.split()))\n",
        "\n",
        "graph_df = pd.DataFrame() \n",
        "\n",
        "graph_df['text'] = text_count\n",
        "graph_df['summary'] = summary_count\n",
        "\n",
        "graph_df.hist(bins = 5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAP4pUC1bxcB"
      },
      "source": [
        "From the graphs, you can determine the range where the maximum number of words fall into. For summary, you can assign the range to be 0-15.\n",
        "\n",
        "To find the range of text which we aren't able to clearly decipher from the graph, consider a random range and find the percentage of words falling into that range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6FCXIiwbvav"
      },
      "outputs": [],
      "source": [
        "# Check how much % of text have 0-100 words\n",
        "cnt = 0\n",
        "for i in pre['cleaned_text']:\n",
        "    if len(i.split()) <= 100:\n",
        "        cnt = cnt + 1\n",
        "print(cnt / len(pre['cleaned_text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbqKEvFab2ZA"
      },
      "source": [
        "As you can observe, 95% of the text pieces fall into the 0-100 category.\n",
        "\n",
        "Now initialize the maximum permissible lengths of both text and summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtU5dP1wbzks"
      },
      "outputs": [],
      "source": [
        "# Model to summarize the text between 0-15 words for Summary and 0-100 words for Text\n",
        "max_text_len = 100\n",
        "max_summary_len = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bol4AMWdb5_C"
      },
      "source": [
        "# Selecting Plausible Texts and Summaries  \n",
        "Select texts and summaries which are below the maximum lengths as defined in previous step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRx1Xud0b409"
      },
      "outputs": [],
      "source": [
        "# Select the Summaries and Text which fall below max length \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "cleaned_text = np.array(pre['cleaned_text'])\n",
        "cleaned_summary= np.array(pre['cleaned_summary'])\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len:\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "post_pre = pd.DataFrame({'text': short_text,'summary': short_summary})\n",
        "\n",
        "post_pre.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4uUhRrgcIBY"
      },
      "source": [
        "Now add start of the sequence (sostok) and end of the sequence (eostok) to denote start and end of the summaries, respectively. This shall be useful to trigger the start of summarization during the inferencing phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Zv4YK6ncDTu"
      },
      "outputs": [],
      "source": [
        "# Add sostok and eostok\n",
        "\n",
        "post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x + ' eostok')\n",
        "\n",
        "post_pre.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XeeofvacNFL"
      },
      "source": [
        "# Text Tokenization  \n",
        "First split the data into train and test data chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgZYyzyIcKEf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(np.array(post_pre[\"text\"]),\n",
        "                                            np.array(post_pre[\"summary\"]),\n",
        "                                            test_size=0.1,\n",
        "                                            random_state=0,\n",
        "                                            shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZsdHcj4cdE6"
      },
      "source": [
        "Prepare and tokenize the text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e1UIEaNcadW"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text to get the vocab count \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare a tokenizer on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAo5TlEBcgNi"
      },
      "source": [
        "Find the percentage of occurrence of rare words (say, occurring less than 5 times) in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tVkfV1Hce9S"
      },
      "outputs": [],
      "source": [
        "thresh = 5\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < thresh:\n",
        "        cnt = cnt + 1\n",
        "    \n",
        "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ats2VS-SckKD"
      },
      "source": [
        "Tokenize the text again by considering the total number of words minus the rare occurrences. Convert text to numbers and pad them all to the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ynok8CsqciHy"
      },
      "outputs": [],
      "source": [
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "# Convert text sequences to integer sequences \n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "# Pad zero upto maximum length\n",
        "x_tr = pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "x_voc = x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLKS_OaNcnvK"
      },
      "source": [
        "Do the same for the summaries as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOHbmn2BcmhB"
      },
      "outputs": [],
      "source": [
        "# Prepare a tokenizer on testing data\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "thresh = 5\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < thresh:\n",
        "        cnt = cnt + 1\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
        "\n",
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "# Convert text sequences to integer sequences \n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "# Pad zero upto maximum length\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "y_voc = y_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klvvzlVbcyhn"
      },
      "source": [
        "# Removing Empty Texts and Summaries  \n",
        "Remove all empty summaries (which only have START and END tokens) and their associated texts from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6kOVw7Mcp31"
      },
      "outputs": [],
      "source": [
        "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
        "ind = []\n",
        "\n",
        "for i in range(len(y_tr)):\n",
        "    cnt = 0\n",
        "    for j in y_tr[i]:\n",
        "        if j != 0:\n",
        "            cnt = cnt + 1\n",
        "    if cnt == 2:\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr = np.delete(y_tr, ind, axis=0)\n",
        "x_tr = np.delete(x_tr, ind, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyvO_0Vwc4IU"
      },
      "source": [
        "Repeat the same for the validation data as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzr68uvBc2lg"
      },
      "outputs": [],
      "source": [
        "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
        "ind = []\n",
        "for i in range(len(y_val)):\n",
        "    cnt = 0\n",
        "    for j in y_val[i]:\n",
        "        if j != 0:\n",
        "            cnt = cnt + 1\n",
        "    if cnt == 2:\n",
        "        ind.append(i)\n",
        "\n",
        "y_val = np.delete(y_val, ind, axis=0)\n",
        "x_val = np.delete(x_val, ind, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKugAi6Pc_0X"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlmhB32nc5Ul"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, \\\n",
        "    Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja_44S_ZdJh7"
      },
      "source": [
        "Define encoder - decoder network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7o6CjRNdCBy"
      },
      "outputs": [],
      "source": [
        "latent_dim = 300\n",
        "embedding_dim = 200\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len, ))\n",
        "\n",
        "# Embedding layer\n",
        "enc_emb = Embedding(x_voc, embedding_dim,\n",
        "                    trainable=True)(encoder_inputs)\n",
        "\n",
        "# Encoder LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
        "\n",
        "# Encoder LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# Encoder LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
        "                     return_sequences=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "# Embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "                    return_state=True, dropout=0.4,\n",
        "                    recurrent_dropout=0.2)\n",
        "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
        "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGthzL1_dNe0"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E3GMarwdIgz"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgTfgYXSdPYE"
      },
      "outputs": [],
      "source": [
        "history = model.fit([x_tr, y_tr[:, :-1]],\n",
        "                    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
        "                    epochs=50,\n",
        "                    callbacks=[es],\n",
        "                    batch_size=128,\n",
        "                    validation_data=([x_val, y_val[:, :-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]),)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFSGZG4Zddut"
      },
      "source": [
        "Next, plot the training and validation loss metrics observed during the training phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJwaXazSdQ0A"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbCOX75kdgaZ"
      },
      "source": [
        "# Generating Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbHx7aiVdfSA"
      },
      "outputs": [],
      "source": [
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2ofKqNydj4n"
      },
      "outputs": [],
      "source": [
        "# Inference Models\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input, \n",
        "                                          decoder_state_input_h, decoder_state_input_c], \n",
        "                      [decoder_outputs2] + [state_h2, state_c2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyFhN0Vodxo4"
      },
      "source": [
        "Now define a function `decode_sequence()` which accepts the input text and outputs the predicted summary. Start with sostok and continue generating words until eostok is encountered or the maximum length of the summary is reached. Predict the upcoming word from a given word by choosing the word which has the maximum probability attached and update the internal state of the decoder accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRZFuuIydlZH"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
        "                + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        if sampled_token != 'eostok':\n",
        "            decoded_sentence += ' ' + sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find the stop word.\n",
        "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
        "            >= max_summary_len - 1:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        (e_h, e_c) = (h, c)\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1vS-6ULd4Sn"
      },
      "source": [
        "Define two functions - `seq2summary()` and `seq2text()` which convert numeric-representation to string-representation of summary and text respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kotv_fCd1aF"
      },
      "outputs": [],
      "source": [
        "# To convert sequence to summary\n",
        "def seq2summary(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0 and i != target_word_index['sostok'] and i \\\n",
        "            != target_word_index['eostok']:\n",
        "            newString = newString + reverse_target_word_index[i] + ' '\n",
        "\n",
        "    return newString\n",
        "\n",
        "\n",
        "# To convert sequence to text\n",
        "def seq2text(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0:\n",
        "            newString = newString + reverse_source_word_index[i] + ' '\n",
        "\n",
        "    return newString"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df43SSgdd-yJ"
      },
      "source": [
        "Finally, generate the predictions by sending in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "4q5xsNAId9BK",
        "outputId": "b35e11d3-e0bd-4dc5-9d3d-22c055a13b96"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-4e968c18c2c0>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print '\\n'\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('\\n')?\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, 19):\n",
        "    print ('Review:', seq2text(x_tr[i]))\n",
        "    print ('Original summary:', seq2summary(y_tr[i]))\n",
        "    print ('Predicted summary:', decode_sequence(x_tr[i].reshape(1,\n",
        "           max_text_len)))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPDuspN8eAE-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
