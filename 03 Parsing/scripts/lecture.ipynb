{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ChartParser\n",
    "from nltk.corpus import treebank\n",
    "import string\n",
    "import nltk\n",
    "from nltk import parse, tokenize, Tree, in_idle \n",
    "from nltk.draw.util import * \n",
    "from nltk.draw.tree import * \n",
    "from nltk.draw.cfg import * \n",
    "\n",
    "\n",
    "groucho_grammar = nltk.CFG.fromstring(\n",
    "    \"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP\n",
    "    NP -> Det N | Det N PP | 'I'\n",
    "    VP -> V NP | VP PP\n",
    "    Det -> 'an' | 'my'\n",
    "    N -> 'elephant' | 'pajamas'\n",
    "    V -> 'shot'\n",
    "    P -> 'in'\n",
    "    \"\"\")\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning... Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,208.0,168.0\" width=\"208px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"19.2308%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.61538%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"19.2308%\" x=\"19.2308%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ate</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.8462%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"61.5385%\" x=\"38.4615%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"43.75%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">white</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.875%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"56.25%\" x=\"43.75%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">chicken</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.875%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.2308%\" y1=\"1.2em\" y2=\"3em\" /></svg>",
      "text/plain": [
       "Tree('S', [('I', 'PRP'), ('ate', 'VBP'), Tree('NP', [('white', 'JJ'), ('chicken', 'NN')])])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "text = 'I ate white chicken'\n",
    "pattern = \"\"\"\n",
    "            NP: {<DT>?<JJ>*<NN>}\n",
    "            VBD:{<VBD>}\n",
    "            IN: {<IN>}\n",
    "        \"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(pattern)\n",
    "result = NPChunker.parse(pos_tag(word_tokenize(text)))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import stanford\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "# from nltk.parse import triples \n",
    "\n",
    "# for governor, dep, dependent in parse.triples():\n",
    "    # print(governor, dep, dependent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88',\n",
       " 'in New York.  Please buy me',\n",
       " 'two of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "s.split() \n",
    "s.split(' ') \n",
    "s.split('\\n') \n",
    "\n",
    "from nltk.tokenize.simple import CharTokenizer\n",
    "from nltk.tokenize import LineTokenizer\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "LineTokenizer(blanklines='keep').tokenize(s) \n",
    "LineTokenizer(blanklines='discard').tokenize(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Mary) (VP (V saw) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "\n",
    "sent = \"Mary saw Bob\".split()\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in rd_parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WFST 1    2    3    4    5    6    7   \n",
      "0    NP   .    .    .    .    .    .    \n",
      "1    .    V    .    .    .    .    .    \n",
      "2    .    .    Det  .    .    .    .    \n",
      "3    .    .    .    N    .    .    .    \n",
      "4    .    .    .    .    P    .    .    \n",
      "5    .    .    .    .    .    Det  .    \n",
      "6    .    .    .    .    .    .    N    \n"
     ]
    }
   ],
   "source": [
    "def init_wfst(tokens, grammar):\n",
    "    numtokens = len(tokens)\n",
    "    wfst = [[None for i in range(numtokens+1)] for j in range(numtokens+1)]\n",
    "    for i in range(numtokens):\n",
    "        productions = grammar.productions(rhs=tokens[i])\n",
    "        wfst[i][i+1] = productions[0].lhs()\n",
    "    return wfst\n",
    "\n",
    "def complete_wfst(wfst, tokens, grammar, trace=False):\n",
    "    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())\n",
    "    numtokens = len(tokens)\n",
    "    for span in range(2, numtokens+1):\n",
    "        for start in range(numtokens+1-span):\n",
    "            end = start + span\n",
    "            for mid in range(start+1, end):\n",
    "                nt1, nt2 = wfst[start][mid], wfst[mid][end]\n",
    "                if nt1 and nt2 and (nt1,nt2) in index:\n",
    "                    wfst[start][end] = index[(nt1,nt2)]\n",
    "                    if trace:\n",
    "                        print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % \\\n",
    "                        (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end))\n",
    "    return wfst\n",
    "\n",
    "def display(wfst, tokens):\n",
    "    print('\\nWFST ' + ' '.join((\"%-4d\" % i) for i in range(1, len(wfst))))\n",
    "    for i in range(len(wfst)-1):\n",
    "        print(\"%d   \" % i, end=\" \")\n",
    "        for j in range(1, len(wfst)):\n",
    "            print(\"%-4s\" % (wfst[i][j] or '.'), end=\" \")\n",
    "        print()\n",
    "\n",
    "tokens = \"I shot an elephant in my pajamas\".split()\n",
    "wfst0 = init_wfst(tokens, groucho_grammar)\n",
    "display(wfst0, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WFST 1    2    3    4    5    6    7   \n",
      "0    NP   .    .    S    .    .    S    \n",
      "1    .    V    .    VP   .    .    VP   \n",
      "2    .    .    Det  NP   .    .    .    \n",
      "3    .    .    .    N    .    .    .    \n",
      "4    .    .    .    .    P    .    PP   \n",
      "5    .    .    .    .    .    Det  NP   \n",
      "6    .    .    .    .    .    .    N    \n"
     ]
    }
   ],
   "source": [
    "wfst1 = complete_wfst(wfst0, tokens, groucho_grammar)\n",
    "display(wfst0, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 7 productions\n",
      "  'shot' -> 'I'\n",
      "  'shot' -> 'elephant'\n",
      "  'shot' -> 'in'\n",
      "  'elephant' -> 'an'\n",
      "  'elephant' -> 'in'\n",
      "  'in' -> 'pajamas'\n",
      "  'pajamas' -> 'my'\n",
      "(shot I (elephant an (in (pajamas my))))\n",
      "(shot I (elephant an) (in (pajamas my)))\n"
     ]
    }
   ],
   "source": [
    "groucho_dep_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
    "... 'shot' -> 'I' | 'elephant' | 'in'\n",
    "... 'elephant' -> 'an' | 'in'\n",
    "... 'in' -> 'pajamas'\n",
    "... 'pajamas' -> 'my'\n",
    "... \"\"\")\n",
    "print(groucho_dep_grammar)\n",
    "\n",
    " \t\n",
    "pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)\n",
    "sent = 'I shot an elephant in my pajamas'.split()\n",
    "trees = pdp.parse(sent)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n"
     ]
    }
   ],
   "source": [
    "sr_parser = nltk.ShiftReduceParser(grammar1)\n",
    "sent = 'Mary saw a dog'.split()\n",
    "for tree in sr_parser.parse(sent):\n",
    "    print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
