{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eOP62Ngbr7h"
      },
      "source": [
        "# Skip-gram word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sVDBYmjheWaT"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tCMJuQnYjCs4"
      },
      "outputs": [],
      "source": [
        "SEED = 2023\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN42rtPOAQF5"
      },
      "source": [
        "## Intuition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_EdVBJfwjgBs"
      },
      "outputs": [],
      "source": [
        "sentence = 'The quick brown fox jumps over the lazy dog'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR4a1RJBwqZI",
        "outputId": "12766622-bec9-4760-a62f-a43c29fa3654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n",
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ],
      "source": [
        "# tokenize and count vocab size\n",
        "tokens = list(sentence.lower().split())\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFEsxXKDajGM"
      },
      "source": [
        "Next, we map the words to numbers. the inverse vocab is dictionary of index as key and vocab as value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrTbVwk9uzCp",
        "outputId": "449c03b1-b018-429e-d71a-1922143ac8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8}\n",
            "{0: '<pad>', 1: 'the', 2: 'quick', 3: 'brown', 4: 'fox', 5: 'jumps', 6: 'over', 7: 'lazy', 8: 'dog'}\n"
          ]
        }
      ],
      "source": [
        "# Create a vocabulary ; tokens to integer indices:\n",
        "vocab, index = {}, 1  # start indexing from 1\n",
        "vocab['<pad>'] = 0  # add a padding token\n",
        "for token in tokens:\n",
        "  if token not in vocab:\n",
        "    vocab[token] = index\n",
        "    index += 1\n",
        "vocab_size = len(vocab)\n",
        "# Create an inverse vocabulary ; integer indices to tokens:\n",
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "print(vocab)\n",
        "print(inverse_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9U7mAvG9oqC",
        "outputId": "b6ffb456-be92-4eb4-a8cd-93af928d3692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ],
      "source": [
        "example_sequence = [vocab[word] for word in tokens]\n",
        "print(example_sequence)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGzyKwDJbAht"
      },
      "source": [
        "The tf.keras.preprocessing.sequence module provides useful functions that simplify data preparation for word2vec. You can use the tf.keras.preprocessing.sequence.skipgrams to generate skip-gram pairs from the example_sequence with a given window_size from tokens in the range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6ey1Xjoh0CA"
      },
      "source": [
        "### positive sample generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dej0YyGbax7"
      },
      "source": [
        "Using a window size of 2, we generate the list of all possible positive training samples given the example sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHIuR7Q4-fPw",
        "outputId": "9797c347-1b46-467e-bb61-a2ae245eca01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        }
      ],
      "source": [
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "                                                                    example_sequence,\n",
        "                                                                    vocabulary_size=vocab_size, \n",
        "                                                                    window_size=window_size, # 2\n",
        "                                                                    negative_samples=0 # library cannot correctly generate negative sample so we set at 0\n",
        "                                                                   )\n",
        "print(len(positive_skip_grams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRJTZK9b-34i",
        "outputId": "313e3c57-9d25-450d-f301-0c4243800027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5, 6): (jumps, over)\n",
            "(7, 1): (lazy, the)\n",
            "(1, 5): (the, jumps)\n",
            "(6, 5): (over, jumps)\n",
            "(3, 1): (brown, the)\n",
            "(5, 1): (jumps, the)\n",
            "(3, 2): (brown, quick)\n",
            "(1, 8): (the, dog)\n",
            "(6, 7): (over, lazy)\n",
            "(7, 8): (lazy, dog)\n"
          ]
        }
      ],
      "source": [
        "for target, context in positive_skip_grams[:10]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pliid1Yuh6Vp"
      },
      "source": [
        "### negative sample generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMAQ_Ahzd03P"
      },
      "source": [
        "The skipgrams function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the tf.random.log_uniform_candidate_sampler function to sample num_ns number of negative samples for a given target word in a window. You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsDqMV7J-646",
        "outputId": "83f73b47-5f67-447e-e138-5834e4617318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([2 1 0 5], shape=(4,), dtype=int64)\n",
            "['quick', 'the', '<pad>', 'jumps']\n"
          ]
        }
      ],
      "source": [
        "# Get target and context words for one positive skip-gram.\n",
        "target_word, context_word = positive_skip_grams[1]\n",
        "\n",
        "# Set the number of negative samples per positive context.\n",
        "num_ns = 4\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
        "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
        "    num_sampled=num_ns,  # number of negative context words to sample\n",
        "    unique=True,  # all the negative samples should be unique\n",
        "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
        "    seed=SEED,  # seed for reproducibility\n",
        "    name=\"negative_sampling\"  # name of this operation\n",
        ")\n",
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iB0yKqneFGC",
        "outputId": "468ea8c2-61d3-4b4e-f4f2-1cf4e2c8fe7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4,)\n",
            "(1, 1)\n"
          ]
        }
      ],
      "source": [
        "print(negative_sampling_candidates.shape)\n",
        "print(context_class.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zSOhJW1vdNI0"
      },
      "outputs": [],
      "source": [
        "# Add a dimension so you can use concatenation (in the next step).\n",
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "\n",
        "# Concatenate a positive context word with negative sampled words.\n",
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "\n",
        "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoybngM2f8WZ",
        "outputId": "1034f488-10aa-4ecd-e0a8-3612b7107549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target_index    : 7\n",
            "target_word     : lazy\n",
            "context_indices : [1 2 1 0 5]\n",
            "context_words   : ['the', 'quick', 'the', '<pad>', 'jumps']\n",
            "label           : [1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Reshape the target to shape `(1,)` and context and label to `(num_ns+1,)`.\n",
        "target = tf.squeeze(target_word)\n",
        "context = tf.squeeze(context)\n",
        "label = tf.squeeze(label)\n",
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M5pECQAp-5Y"
      },
      "source": [
        "# Train Skip-gram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O2UcrcYFf-Ua"
      },
      "outputs": [],
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for `vocab_size` tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in the dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with a positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=seed,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37DRT-oXqUy7"
      },
      "source": [
        "## Load data \n",
        "shakespeare from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5iupFn-iMk7",
        "outputId": "11946dfe-b35a-4bbe-eafe-80c800b6934b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From c:\\Users\\Renan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL0Vrfzbqw2B"
      },
      "source": [
        "data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yiMKOz32qo8j"
      },
      "outputs": [],
      "source": [
        "# lower case and remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Define the vocab size and max sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# create vectorization layer to pre-process data\n",
        "# normalize, split, and map strings to integers.\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "                                            standardize=custom_standardization, # data prep\n",
        "                                            max_tokens=vocab_size, # max tokens\n",
        "                                            output_mode='int',\n",
        "                                            output_sequence_length=sequence_length # pad all samples to the same length.\n",
        "                                           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Kxh3dtupmFme"
      },
      "outputs": [],
      "source": [
        "# Call TextVectorization.adapt on the text dataset to create vocabulary.\n",
        "# batch of 1024\n",
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBW0DJ3hmP4A",
        "outputId": "664e9c09-10bc-40d4-b930-0c272f646d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4096 words in vocab =>  ['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a']\n"
          ]
        }
      ],
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(len(inverse_vocab), 'words in vocab => ' ,inverse_vocab[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p88q6RYtmmDi",
        "outputId": "a6827cd9-54d8-4410-fac9-63718a38d8c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32777\n",
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
            "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
            "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
            "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))\n",
        "for seq in sequences[:5]: # see first 5\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XwJ_td8tCP7"
      },
      "source": [
        "## Generate training examples\n",
        "sequences is now a list of int encoded sentences. Just call the generate_training_data function defined earlier to generate training examples for the word2vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be the same, representing the total number of training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "773cbDO8st_s",
        "outputId": "615751ca-9acb-4865-b29f-32443334a4c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32777/32777 [01:16<00:00, 430.59it/s]\n"
          ]
        }
      ],
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "                                                    sequences=sequences,\n",
        "                                                    window_size=2,\n",
        "                                                    num_ns=4,\n",
        "                                                    vocab_size=vocab_size,\n",
        "                                                    seed=SEED\n",
        "                                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Un1QJItWSm",
        "outputId": "5fe9b7c5-b353-4505-f5c8-bfbd2b15cc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "targets.shape: (65835,)\n",
            "contexts.shape: (65835, 5)\n",
            "labels.shape: (65835, 5)\n"
          ]
        }
      ],
      "source": [
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9sGyd14uI9j"
      },
      "source": [
        "To perform efficient batching for the potentially large number of training examples, use the tf.data.Dataset API. After this step, you would have a tf.data.Dataset object of (target_word, context_word), (label) elements to train your word2vec model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1aiD2b4tsp4",
        "outputId": "d695de5a-2c71-4891-e393-16a13c26ef2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdgtGK2Pub6v"
      },
      "source": [
        "## word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3cqfOs84txEb"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                             embedding_dim,\n",
        "                                             input_length=1,\n",
        "                                             name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                              embedding_dim,\n",
        "                                              input_length=num_ns+1)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
        "    # context: (batch, context)\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)\n",
        "    # word_emb: (batch, embed)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    # context_emb: (batch, context, embed)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "    # dots: (batch, context)\n",
        "    return dots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_9GQWe4svIeW"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 50\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "64/64 [==============================] - 1s 13ms/step - loss: 0.7831 - accuracy: 0.7707\n",
            "Epoch 2/50\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.7559 - accuracy: 0.7809\n",
            "Epoch 3/50\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.7300 - accuracy: 0.7901\n",
            "Epoch 4/50\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.7054 - accuracy: 0.7991\n",
            "Epoch 5/50\n",
            "64/64 [==============================] - 2s 28ms/step - loss: 0.6819 - accuracy: 0.8073\n",
            "Epoch 6/50\n",
            "64/64 [==============================] - 2s 26ms/step - loss: 0.6595 - accuracy: 0.8150\n",
            "Epoch 7/50\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.6382 - accuracy: 0.8222\n",
            "Epoch 8/50\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.6180 - accuracy: 0.8297\n",
            "Epoch 9/50\n",
            "64/64 [==============================] - 1s 14ms/step - loss: 0.5986 - accuracy: 0.8360\n",
            "Epoch 10/50\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.5802 - accuracy: 0.8421\n",
            "Epoch 11/50\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.5627 - accuracy: 0.8481\n",
            "Epoch 12/50\n",
            "64/64 [==============================] - 1s 14ms/step - loss: 0.5459 - accuracy: 0.8539\n",
            "Epoch 13/50\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.5300 - accuracy: 0.8592\n",
            "Epoch 14/50\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.5147 - accuracy: 0.8648\n",
            "Epoch 15/50\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.5002 - accuracy: 0.8694\n",
            "Epoch 16/50\n",
            "64/64 [==============================] - 1s 13ms/step - loss: 0.4863 - accuracy: 0.8736\n",
            "Epoch 17/50\n",
            "64/64 [==============================] - 1s 13ms/step - loss: 0.4730 - accuracy: 0.8777\n",
            "Epoch 18/50\n",
            "64/64 [==============================] - 1s 14ms/step - loss: 0.4603 - accuracy: 0.8818\n",
            "Epoch 19/50\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4482 - accuracy: 0.8856\n",
            "Epoch 20/50\n",
            "64/64 [==============================] - 1s 13ms/step - loss: 0.4366 - accuracy: 0.8891\n",
            "Epoch 21/50\n",
            "64/64 [==============================] - 1s 14ms/step - loss: 0.4255 - accuracy: 0.8927\n",
            "Epoch 22/50\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4148 - accuracy: 0.8957\n",
            "Epoch 23/50\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.4046 - accuracy: 0.8992\n",
            "Epoch 24/50\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.3949 - accuracy: 0.9024\n",
            "Epoch 25/50\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.3855 - accuracy: 0.9050\n",
            "Epoch 26/50\n",
            "64/64 [==============================] - 1s 14ms/step - loss: 0.3765 - accuracy: 0.9076\n",
            "Epoch 27/50\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.3678 - accuracy: 0.9103\n",
            "Epoch 28/50\n",
            "64/64 [==============================] - 1s 20ms/step - loss: 0.3595 - accuracy: 0.9127\n",
            "Epoch 29/50\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.3515 - accuracy: 0.9149\n",
            "Epoch 30/50\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3438 - accuracy: 0.9170\n",
            "Epoch 31/50\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.3364 - accuracy: 0.9190\n",
            "Epoch 32/50\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.3293 - accuracy: 0.9211\n",
            "Epoch 33/50\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3224 - accuracy: 0.9225\n",
            "Epoch 34/50\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3158 - accuracy: 0.9243\n",
            "Epoch 35/50\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.3095 - accuracy: 0.9261\n",
            "Epoch 36/50\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3033 - accuracy: 0.9275\n",
            "Epoch 37/50\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2974 - accuracy: 0.9293\n",
            "Epoch 38/50\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 0.2917 - accuracy: 0.9308\n",
            "Epoch 39/50\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.2862 - accuracy: 0.9324\n",
            "Epoch 40/50\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2808 - accuracy: 0.9335\n",
            "Epoch 41/50\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2757 - accuracy: 0.9349\n",
            "Epoch 42/50\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2707 - accuracy: 0.9360\n",
            "Epoch 43/50\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2659 - accuracy: 0.9369\n",
            "Epoch 44/50\n",
            "64/64 [==============================] - 1s 20ms/step - loss: 0.2613 - accuracy: 0.9381\n",
            "Epoch 45/50\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.2568 - accuracy: 0.9392\n",
            "Epoch 46/50\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2524 - accuracy: 0.9404\n",
            "Epoch 47/50\n",
            "64/64 [==============================] - 1s 14ms/step - loss: 0.2482 - accuracy: 0.9415\n",
            "Epoch 48/50\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.2441 - accuracy: 0.9423\n",
            "Epoch 49/50\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.2401 - accuracy: 0.9434\n",
            "Epoch 50/50\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.2363 - accuracy: 0.9444\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2789b068f40>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2vec.fit(dataset, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rK-ZzgJwB0N"
      },
      "source": [
        "# save word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xaHqohoXv2Ci"
      },
      "outputs": [],
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFYQEKBmwQeL",
        "outputId": "a69d8ef3-a733-4450-89a3-70d91284ef4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i'] [[-0.21401028  0.21716307 -0.17976478 -0.6316895   0.10166845  0.252107\n",
            "   0.7220733   0.27261996 -0.41566148 -0.00573596  0.00827512  0.06708087\n",
            "  -0.71460193 -0.7662184  -0.20834157  1.2057359  -0.28015885 -0.23546633\n",
            "  -0.09241361 -0.21304241 -0.11482741  0.7592964   0.1791624  -0.47790322\n",
            "  -0.7363419   0.37624288  0.5388307   0.41706997  0.5708758  -0.03669158\n",
            "  -0.42881346 -0.2034348   0.05349772 -0.32028213  0.11303154  0.85656285\n",
            "  -0.09738274  0.0042937  -0.06563117 -0.66657156  0.27259076 -0.17700285\n",
            "   0.31220567 -0.12347134 -0.50662225  0.19906326 -0.3557012   0.16824552\n",
            "   0.570906    0.06169809]]\n"
          ]
        }
      ],
      "source": [
        "print(vocab[5:6], weights[2:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "E4C3JWg2wOhN"
      },
      "outputs": [],
      "source": [
        "out_v = io.open('emb_vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('vocab.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.21401028,  0.21716307, -0.17976478, -0.6316895 ,  0.10166845,\n",
              "         0.252107  ,  0.7220733 ,  0.27261996, -0.41566148, -0.00573596,\n",
              "         0.00827512,  0.06708087, -0.71460193, -0.7662184 , -0.20834157,\n",
              "         1.2057359 , -0.28015885, -0.23546633, -0.09241361, -0.21304241,\n",
              "        -0.11482741,  0.7592964 ,  0.1791624 , -0.47790322, -0.7363419 ,\n",
              "         0.37624288,  0.5388307 ,  0.41706997,  0.5708758 , -0.03669158,\n",
              "        -0.42881346, -0.2034348 ,  0.05349772, -0.32028213,  0.11303154,\n",
              "         0.85656285, -0.09738274,  0.0042937 , -0.06563117, -0.66657156,\n",
              "         0.27259076, -0.17700285,  0.31220567, -0.12347134, -0.50662225,\n",
              "         0.19906326, -0.3557012 ,  0.16824552,  0.570906  ,  0.06169809]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights[2:3]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
