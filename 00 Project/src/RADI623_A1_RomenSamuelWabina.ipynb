{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RADI623: Natural Language Processing**\n",
    "\n",
    "### Assignment: Natural Language Processing\n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University\n",
    "\n",
    "Note: In case of Python Markdown errors, you may access the assignment through this GitHub [Link](https://github.com/rrwabina/RADI605/tree/main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Medical Specialty Identification**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting oneâ€™s illnesses wrongly through self-diagnosis in medicine is very real. In a report by the [Telegraph](https://www.telegraph.co.uk/news/health/news/11760658/One-in-four-self-diagnose-on-the-internet-instead-of-visiting-the-doctor.html), nearly one in four self-diagnose instead of visiting the doctor. Out of those who misdiagnose, nearly half have misdiagnosed their illness wrongly [reported](https://bigthink.com/health/self-diagnosis/). While there could be multiple root causes to this problem, this could stem from a general unwillingness and inability to seek professional help.\n",
    "\n",
    "Elevent percent of the respondents surveyed, for example, could not find an appointment in time. This means that crucial time is lost during the screening phase of a medical treatment, and early diagnosis which could have resulted in illnesses treated earlier was not achieved.\n",
    "\n",
    "With the knowledge of which medical specialty area to focus on, a patient can receive targeted help much faster through consulting specialist doctors. To alleviate waiting times and predict which area of medical specialty to focus on, we can utilize natural language processing (NLP) to solve this task.\n",
    "\n",
    "Given any medical transcript or patient condition, this solution would predict the medical specialty that the patient should seek help in. Ideally, given a sufficiently comprehensive transcript (and dataset), one would be able to predict exactly which illness he is suffering from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import spacy\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.pipeline.tagger import Tagger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/mtsamples.csv')\n",
    "\n",
    "num_samples = len(data)\n",
    "num_medical_specialties = data['medical_specialty'].nunique()\n",
    "\n",
    "def calculate_univariate(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    avg_description_length = description_lengths.mean()\n",
    "    min_description_length = description_lengths.min()\n",
    "    max_description_length = description_lengths.max()\n",
    "\n",
    "    avg_transcription_length = transcription_lengths.mean()\n",
    "    min_transcription_length = transcription_lengths.min()\n",
    "    max_transcription_length = transcription_lengths.max()\n",
    "\n",
    "    dictionary = {}\n",
    "    dictionary['description']   = [avg_description_length, min_description_length, max_description_length]\n",
    "    dictionary['transcription'] = [avg_transcription_length, min_transcription_length, max_transcription_length]\n",
    "    return dictionary\n",
    "summary = calculate_univariate(data)\n",
    "\n",
    "def plot_classes(data):\n",
    "    specialty_counts = data['medical_specialty'].value_counts()\n",
    "\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    plt.bar(specialty_counts.index, specialty_counts.values)\n",
    "    plt.xlabel('Medical Specialty')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Medical Specialties')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].hist(description_lengths, bins=50, alpha=0.8)\n",
    "    axs[0].set_xlabel('Description Length')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].set_title('Histogram of Description Lengths')\n",
    "\n",
    "    axs[1].hist(transcription_lengths, bins = 50, alpha = 0.8)\n",
    "    axs[1].set_xlabel('Transcription Length')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].set_title('Histogram of Transcription Lengths')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description             0\n",
       "medical_specialty       0\n",
       "sample_name             0\n",
       "transcription           0\n",
       "keywords             1068\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['transcription'].fillna(data['description'], inplace = True)\n",
    "len(data['medical_specialty'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in transcriptions column: 140259\n",
      "Number of unique words in transcriptions column: 35807\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_word_count(text_list):\n",
    "    sent_count = 0\n",
    "    word_count = 0\n",
    "    vocab = {}\n",
    "    for text in text_list:\n",
    "        sentences=sent_tokenize(str(text).lower())\n",
    "        sent_count = sent_count + len(sentences)\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                if(word in vocab.keys()):\n",
    "                    vocab[word] = vocab[word] +1\n",
    "                else:\n",
    "                    vocab[word] = 1 \n",
    "    word_count = len(vocab.keys())\n",
    "    return sent_count,word_count\n",
    "\n",
    "clinical_text_df = data[data['transcription'].notna()]\n",
    "sent_count, word_count = get_sentence_word_count(clinical_text_df['transcription'].tolist())\n",
    "\n",
    "print('Number of sentences in transcriptions column: '    + str(sent_count))\n",
    "print('Number of unique words in transcriptions column: ' + str(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in transcriptions column: 130924\n",
      "Number of unique words in transcriptions column: 35090\n"
     ]
    }
   ],
   "source": [
    "data_categories  = clinical_text_df.groupby(clinical_text_df['medical_specialty'])\n",
    "filtered_data_categories = data_categories.filter(lambda x:x.shape[0] > 50)\n",
    "final_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\n",
    "sent_count, word_count = get_sentence_word_count(filtered_data_categories['transcription'].tolist())\n",
    "\n",
    "print('Number of sentences in transcriptions column: '    + str(sent_count))\n",
    "print('Number of unique words in transcriptions column: ' + str(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = filtered_data_categories[['description', 'medical_specialty', 'sample_name', 'transcription', 'keywords']]\n",
    "reduced_df = reduced_df.drop(reduced_df[reduced_df['transcription'].isna()].index)\n",
    "reduced_df.to_csv('../data/mtsamples_modified.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>preprocessing</code> function takes a sentence, removes hyperlinks, performs various token-level filters (removing stop words, symbols, punctuation marks, and whitespace), lemmatizes the remaining tokens to their base forms, and returns the cleaned sentence as a string. Specifically, the code <code>token.pos_ != 'SYM' and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE'</code> checks if the token's part-of-speech (POS) tag is not 'SYM' (symbol), 'PUNCT' (punctuation), or 'SPACE'. It further filters out tokens that are symbols, punctuation marks, or represent whitespace. We also appended the lowercase lemma (base form) of the token, obtained using <code>token.lemma_</code>, to the cleaned_tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    if seed:\n",
    "        logging.info(f'Running in deterministic mode with seed {seed}')\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    else:\n",
    "        logging.info('Running in non-deterministic mode')\n",
    "set_seed(2023)\n",
    "\n",
    "def remove_hyperlinks(sentence):\n",
    "    sentence = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"', \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    sentence = remove_hyperlinks(sentence)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and \\\n",
    "            token.pos_ != 'SYM' and \\\n",
    "            token.pos_ != 'PUNCT' and token.pos_ != 'SPACE':\n",
    "            cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], \n",
    "                                 truncation = True, \n",
    "                                 is_split_into_words = True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def to_tokens(tokenizer, sentence):\n",
    "    inputs = tokenizer(sentence)\n",
    "    return tokenizer.convert_ids_to_tokens(inputs.input_ids)\n",
    "\n",
    "def load_preprocessing(path = '../data/mtsamples_modified.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[:50, :]\n",
    "    for i, row in df.iterrows():\n",
    "        df.at[i, 'description']   = preprocessing(row['description'])\n",
    "        df.at[i, 'medical_specialty'] = preprocessing(row['medical_specialty'])\n",
    "        df.at[i, 'sample_name']   = preprocessing(row['sample_name'])\n",
    "        df.at[i, 'transcription'] = preprocessing(row['transcription']) if not pd.isnull(row['transcription']) else np.NaN  \n",
    "        df.at[i, 'keywords']      = preprocessing(row['keywords']) if not pd.isnull(row['keywords']) else np.NaN  \n",
    "    return df\n",
    "\n",
    "def split_data(df):\n",
    "    shuffle = df.sample(frac = 1, random_state = 42)\n",
    "\n",
    "    train_data,  test_data = train_test_split(shuffle,    test_size = 0.30, random_state = 42)\n",
    "    train_data, valid_data = train_test_split(train_data, test_size = 0.15, random_state = 42) \n",
    "\n",
    "    train_data.to_csv('../data/train.csv', index = False)\n",
    "    valid_data.to_csv('../data/valid.csv', index = False)\n",
    "    test_data. to_csv('../data/test.csv' , index = False)\n",
    "\n",
    "    data_files = {\n",
    "        'train': '../data/train.csv',\n",
    "        'valid': '../data/valid.csv',\n",
    "        'test' : '../data/test.csv'}\n",
    "    dataset = load_dataset('csv', data_files = data_files, streaming = True)\n",
    "    return dataset \n",
    "\n",
    "def compute_review_length(example):\n",
    "    return {'review_length': len(example['transcription'].split())}\n",
    "\n",
    "def bert_tokenizer(df, use_special):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    input_ids, attention_masks = [], []\n",
    "\n",
    "    if use_special:\n",
    "        for index, row in df.iterrows():\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                row['description'],\n",
    "                row['medical_specialty'],\n",
    "                row['sample_name'],\n",
    "                row['transcription'],\n",
    "                row['keywords'],\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "\n",
    "    else:\n",
    "        for description in df['description']:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                description,\n",
    "                add_special_tokens = True, \n",
    "                max_length = 512, \n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def process(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"], truncation = True, max_length=512\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_preprocessing()\n",
    "dataset = split_data(df)\n",
    "next(iter(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = df['transcription'].astype('str')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['medical_specialty'])\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_inputs = tokenizer.batch_encode_plus(\n",
    "                    text_column.tolist(),\n",
    "                    max_length = 512,\n",
    "                    padding = 'max_length',\n",
    "                    truncation = True,\n",
    "                    return_attention_mask = True,\n",
    "                    return_tensors = 'pt')\n",
    "\n",
    "input_ids = encoded_inputs['input_ids']\n",
    "attention_mask = encoded_inputs['attention_mask']\n",
    "token_type_ids = encoded_inputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDATASET(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.sequences[index]\n",
    "        label = self.labels[index]\n",
    "        return sequence, label\n",
    "    \n",
    "def GENERATE_DATALOADER(input_ids, attention_mask, labels, batch_size = 64, use_sampler = True):\n",
    "    if use_sampler:\n",
    "        oversampler = RandomOverSampler(random_state = 42)\n",
    "        X = np.concatenate((input_ids, attention_mask), axis = -1)\n",
    "        y = np.ravel(labels)\n",
    "\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "        input_ids_resampled      = X_resampled[:, :input_ids.shape[1]]\n",
    "        attention_mask_resampled = X_resampled[:, input_ids.shape[1]:]\n",
    "        labels_resampled = y_resampled\n",
    "\n",
    "        dataset = TensorDataset(torch.tensor(input_ids_resampled),\n",
    "                                torch.tensor(attention_mask_resampled),\n",
    "                                torch.tensor(labels_resampled))\n",
    "        \n",
    "        train_size = int(0.6 * len(dataset))\n",
    "        valid_size = int(0.2 * len(dataset))\n",
    "        tests_size = len(dataset) - train_size - valid_size\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, tests_size])\n",
    "        \n",
    "    else:\n",
    "        dataset = TensorDataset(torch.tensor(input_ids), \n",
    "                                torch.tensor(attention_mask), \n",
    "                                torch.tensor(labels))\n",
    "        \n",
    "        train_size = int(0.6 * len(dataset))\n",
    "        valid_size = int(0.2 * len(dataset))\n",
    "        tests_size = len(dataset) - train_size - valid_size\n",
    "\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, tests_size])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = batch_size)\n",
    "    validation_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler = SequentialSampler(valid_dataset),\n",
    "        batch_size = batch_size)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler = SequentialSampler(test_dataset),\n",
    "        batch_size = batch_size)\n",
    "    return train_dataloader, validation_dataloader, test_dataloader\n",
    "\n",
    "train_loader, valid_loader, test_loader = GENERATE_DATALOADER(input_ids, attention_mask, labels, use_sampler = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        _, (hidden, _) = self.lstm(inputs)\n",
    "        hidden = hidden.squeeze(0)  \n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "    \n",
    "class BasicClassifier(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, out_features):\n",
    "        super(BasicClassifier, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, out_features)\n",
    "                \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs.squeeze(1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        probs = F.relu(logits)\n",
    "        return probs\n",
    "\n",
    "def BERT_EMBEDDING(input_ids, attention_mask, token_type_ids):\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids = input_ids, \n",
    "                             attention_mask = attention_mask, \n",
    "                             token_type_ids = token_type_ids)\n",
    "        bert_embeddings = outputs.last_hidden_state\n",
    "\n",
    "    batch_size = bert_embeddings.size(0)\n",
    "    sequence_length = bert_embeddings.size(1)\n",
    "    bert_embeddings = bert_embeddings.view(batch_size, sequence_length, -1)\n",
    "    embeddings  = bert_embeddings.permute(1, 0, 2)\n",
    "    return bert_model, embeddings\n",
    "\n",
    "def LSTM_BASELINE(bert_model, embeddings):\n",
    "    input_size = bert_model.config.hidden_size\n",
    "    hidden_size, num_classes = 50, 20  \n",
    "    lstm_model   = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "    lstm_output  = lstm_model(embeddings)\n",
    "    output_probs = nn.functional.softmax(lstm_output, dim = 1)\n",
    "    _, predicted_labels = torch.max(output_probs, dim = 1)\n",
    "    return output_probs, predicted_labels\n",
    "\n",
    "bert_model, embeddings = BERT_EMBEDDING(input_ids, attention_mask, token_type_ids)\n",
    "output_probs, predicted_labels = LSTM_BASELINE(bert_model, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.0, 'Precision': 0.0, 'Recall': 0.0, 'F1-score': 0.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def accuracy(preds, y):\n",
    "    predicted = torch.max(preds.data, 1)[1]\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    return acc\n",
    "\n",
    "def evaluate_predictions(predictions, labels):\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "    true_labels = labels.numpy()\n",
    "\n",
    "    accuracy  = accuracy_score(true_labels, \n",
    "                               predicted_labels)\n",
    "    \n",
    "    precision = precision_score(true_labels, \n",
    "                               predicted_labels, \n",
    "                               average = 'weighted')\n",
    "    \n",
    "    recall = recall_score(true_labels, \n",
    "                          predicted_labels, \n",
    "                          average = 'weighted')\n",
    "    \n",
    "    f1 = f1_score(true_labels, \n",
    "                  predicted_labels, \n",
    "                  average = 'weighted')\n",
    "    return {\n",
    "        'Accuracy':     np.round(accuracy, 4),\n",
    "        'Precision':    np.round(precision, 4),\n",
    "        'Recall':       np.round(recall, 4),\n",
    "        'F1-score':     np.round(f1, 4)}\n",
    "        \n",
    "evaluate_predictions(output_probs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() \n",
    "    \n",
    "    for i, (label, text) in enumerate(loader): \n",
    "        label = label.to(device) \n",
    "        text = text.to(device) \n",
    "\n",
    "        predictions = model(text).squeeze(1) \n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()            \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length\n",
    "\n",
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (label, text) in enumerate(loader): \n",
    "            label = label.to(device) \n",
    "            text  = text.to(device)  \n",
    "\n",
    "            predictions = model(text).squeeze(1) \n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_custom_vocabs(path = '../data/SNMI.csv'):\n",
    "    clinical = pd.read_csv(path)\n",
    "    features = ['Preferred Label','Synonyms']\n",
    "    clinical = clinical[features]\n",
    "    clinical = clinical['Preferred Label'].append(clinical['Synonyms'])\n",
    "    clinical = clinical.dropna()\n",
    "    \n",
    "    vocab = clinical.str.split('\\W', expand = True).stack().unique()\n",
    "    vocab = filter(None, vocab)\n",
    "\n",
    "    filepath = '../data/vocab.txt'\n",
    "    with open(filepath, 'w') as file_handler:\n",
    "        for item in vocab:\n",
    "            file_handler.write('{}\\n'.format(item))\n",
    "\n",
    "    with open('../data/vocab.txt', 'r') as f:\n",
    "        vocab_words = set(f.read().splitlines())\n",
    "    return vocab_words  \n",
    "\n",
    "vocab_words = initialize_custom_vocabs()\n",
    "def is_vocab_word(token):\n",
    "    return token.lower_ in vocab_words\n",
    "\n",
    "execute = True\n",
    "if execute:\n",
    "    spacy.tokens.Token.set_extension('is_vocab', getter = is_vocab_word, force = True)\n",
    "\n",
    "def medical_vocabs(text):\n",
    "    doc = nlp(text)\n",
    "    tagged_tokens = [(token.text, token.pos_) for token in doc]\n",
    "    filtered_tokens = [(token, pos) for token, pos in tagged_tokens if token._.is_vocab]\n",
    "    return filtered_tokens\n",
    "\n",
    "for word in vocab_words:\n",
    "    nlp.vocab[word]\n",
    "vocab = nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index2word(vocab_words):\n",
    "    word2index = {'<PAD>': 0, \n",
    "                  '<UNK>': 1}\n",
    "    for vo in vocab_words:\n",
    "        if word2index.get(vo) is None:\n",
    "            word2index[vo] = len(word2index)\n",
    "            \n",
    "    index2word = {v:k for k, v in word2index.items()}\n",
    "    return index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_custom_tagger(path = '../data/clinical-stopwords.txt'):\n",
    "    with open(path, 'r') as f:\n",
    "        stop_words = set(f.read().splitlines())\n",
    "    return stop_words\n",
    "\n",
    "stop_words = initialize_custom_tagger()\n",
    "def is_stop_word(token):\n",
    "    return token.lower_ in stop_words\n",
    "\n",
    "execute = True\n",
    "if execute:\n",
    "    spacy.tokens.Token.set_extension('is_stop', getter = is_stop_word, force = True)\n",
    "\n",
    "def medical_tagger(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.lower_ in stop_words:\n",
    "            token.is_stop = True\n",
    "        else:\n",
    "            token.is_stop = False\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tagger  = Tagger(nlp.vocab, medical_tagger)\n",
    "vocab_tagger = Tagger(nlp.vocab, medical_vocabs)\n",
    "excluded_tokens = {}\n",
    "\n",
    "\n",
    "use_stop_tagger, use_vocab_tagger  = False, False\n",
    "if use_stop_tagger:\n",
    "    nlp.add_pipe('stop_tagger', config = {'component': stop_tagger}, last = True)\n",
    "    excluded_tokens.add('is_stop')\n",
    "\n",
    "\n",
    "if use_vocab_tagger:\n",
    "    nlp.add_pipe(name = 'vocab tagger',\n",
    "                 component = vocab_tagger,\n",
    "                 remote = True\n",
    "                )\n",
    "    excluded_tokens['is_vocab'] = {False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/mtsamples.csv')\n",
    "dataset['transcription'].fillna(dataset['description'], inplace = True)\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000]['transcription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000]['transcription']\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token = '[UNK]'))\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), \n",
    "                                             normalizers.Lowercase(), \n",
    "                                             normalizers.StripAccents()])\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "\n",
    "special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "trainer = trainers.WordPieceTrainer(vocab_size = 25000, special_tokens = special_tokens)\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer = trainer)\n",
    "\n",
    "cls_token_id = tokenizer.token_to_id('[CLS]')\n",
    "sep_token_id = tokenizer.token_to_id('[SEP]')\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single = f'[CLS]:0 $A:0 [SEP]:0',\n",
    "    pair   = f'[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1',\n",
    "    special_tokens = [('[CLS]', cls_token_id), ('[SEP]', sep_token_id)])\n",
    "tokenizer.decoder = decoders.WordPiece(prefix = '##')\n",
    "tokenizer.save('../data/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GENERATE_NEW_TOKENIZER:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\n",
    "        self.tokenizer.normalizer = normalizers.Sequence([\n",
    "            normalizers.NFD(),\n",
    "            normalizers.Lowercase(),\n",
    "            normalizers.StripAccents()])\n",
    "        \n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "        self.special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "        self.trainer = trainers.WordPieceTrainer(\n",
    "            vocab_size = 50000,\n",
    "            special_tokens = self.special_tokens)\n",
    "        self.cls_token_id  = None\n",
    "        self.sep_token_id  = None\n",
    "\n",
    "    def get_training_corpus(self, dataset):\n",
    "        for i in range(0, len(dataset), 1000):\n",
    "            yield dataset[i: i + 1000]['transcription']\n",
    "\n",
    "    def train_tokenizer(self, dataset):\n",
    "        self.tokenizer.train_from_iterator(\n",
    "            self.get_training_corpus(dataset),\n",
    "            trainer = self.trainer)\n",
    "        self.cls_token_id = self.tokenizer.token_to_id('[CLS]')\n",
    "        self.sep_token_id = self.tokenizer.token_to_id('[SEP]')\n",
    "\n",
    "        self.tokenizer.post_processor = processors.TemplateProcessing(\n",
    "            single = f'[CLS]:0 $A:0 [SEP]:0',\n",
    "            pair   = f'[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1',\n",
    "            special_tokens=[('[CLS]', self.cls_token_id), ('[SEP]', self.sep_token_id)])\n",
    "        self.tokenizer.decoder = decoders.WordPiece(prefix='##')\n",
    "\n",
    "    def save_tokenizer(self, filepath):\n",
    "        self.tokenizer.save(filepath)\n",
    "        \n",
    "tokenizer = GENERATE_NEW_TOKENIZER()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
