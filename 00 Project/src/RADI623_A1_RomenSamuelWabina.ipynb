{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RADI623: Natural Language Processing**\n",
    "\n",
    "### Assignment: Natural Language Processing\n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University\n",
    "\n",
    "Note: In case of Python Markdown errors, you may access the assignment through this GitHub [Link](https://github.com/rrwabina/RADI605/tree/main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Medical Specialty Identification**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting oneâ€™s illnesses wrongly through self-diagnosis in medicine is very real. In a report by the [Telegraph](https://www.telegraph.co.uk/news/health/news/11760658/One-in-four-self-diagnose-on-the-internet-instead-of-visiting-the-doctor.html), nearly one in four self-diagnose instead of visiting the doctor. Out of those who misdiagnose, nearly half have misdiagnosed their illness wrongly [reported](https://bigthink.com/health/self-diagnosis/). While there could be multiple root causes to this problem, this could stem from a general unwillingness and inability to seek professional help.\n",
    "\n",
    "Elevent percent of the respondents surveyed, for example, could not find an appointment in time. This means that crucial time is lost during the screening phase of a medical treatment, and early diagnosis which could have resulted in illnesses treated earlier was not achieved.\n",
    "\n",
    "With the knowledge of which medical specialty area to focus on, a patient can receive targeted help much faster through consulting specialist doctors. To alleviate waiting times and predict which area of medical specialty to focus on, we can utilize natural language processing (NLP) to solve this task.\n",
    "\n",
    "Given any medical transcript or patient condition, this solution would predict the medical specialty that the patient should seek help in. Ideally, given a sufficiently comprehensive transcript (and dataset), one would be able to predict exactly which illness he is suffering from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import spacy\n",
    "import re\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/mtsamples.csv')\n",
    "\n",
    "num_samples = len(data)\n",
    "num_medical_specialties = data['medical_specialty'].nunique()\n",
    "\n",
    "def calculate_univariate(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    avg_description_length = description_lengths.mean()\n",
    "    min_description_length = description_lengths.min()\n",
    "    max_description_length = description_lengths.max()\n",
    "\n",
    "    avg_transcription_length = transcription_lengths.mean()\n",
    "    min_transcription_length = transcription_lengths.min()\n",
    "    max_transcription_length = transcription_lengths.max()\n",
    "\n",
    "    dictionary = {}\n",
    "    dictionary['description']   = [avg_description_length, min_description_length, max_description_length]\n",
    "    dictionary['transcription'] = [avg_transcription_length, min_transcription_length, max_transcription_length]\n",
    "    return dictionary\n",
    "summary = calculate_univariate(data)\n",
    "\n",
    "def plot_classes(data):\n",
    "    specialty_counts = data['medical_specialty'].value_counts()\n",
    "\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    plt.bar(specialty_counts.index, specialty_counts.values)\n",
    "    plt.xlabel('Medical Specialty')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Medical Specialties')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].hist(description_lengths, bins=50, alpha=0.8)\n",
    "    axs[0].set_xlabel('Description Length')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].set_title('Histogram of Description Lengths')\n",
    "\n",
    "    axs[1].hist(transcription_lengths, bins = 50, alpha = 0.8)\n",
    "    axs[1].set_xlabel('Transcription Length')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].set_title('Histogram of Transcription Lengths')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>preprocessing</code> function takes a sentence, removes hyperlinks, performs various token-level filters (removing stop words, symbols, punctuation marks, and whitespace), lemmatizes the remaining tokens to their base forms, and returns the cleaned sentence as a string. Specifically, the code <code>token.pos_ != 'SYM' and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE'</code> checks if the token's part-of-speech (POS) tag is not 'SYM' (symbol), 'PUNCT' (punctuation), or 'SPACE'. It further filters out tokens that are symbols, punctuation marks, or represent whitespace. We also appended the lowercase lemma (base form) of the token, obtained using <code>token.lemma_</code>, to the cleaned_tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks(sentence):\n",
    "    sentence = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"', \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    sentence = remove_hyperlinks(sentence)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and \\\n",
    "            token.pos_ != 'SYM' and \\\n",
    "            token.pos_ != 'PUNCT' and token.pos_ != 'SPACE':\n",
    "            cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], \n",
    "                                 truncation = True, \n",
    "                                 is_split_into_words = True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def to_tokens(tokenizer, sentence):\n",
    "    inputs = tokenizer(sentence)\n",
    "    return tokenizer.convert_ids_to_tokens(inputs.input_ids)\n",
    "\n",
    "def load_preprocessing(path = '../data/mtsamples.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    for i, row in df.iterrows():\n",
    "        df.at[i, 'description']   = preprocessing(row['description'])\n",
    "        df.at[i, 'medical_specialty'] = preprocessing(row['medical_specialty'])\n",
    "        df.at[i, 'sample_name']   = preprocessing(row['sample_name'])\n",
    "        df.at[i, 'transcription'] = preprocessing(row['transcription']) if not pd.isnull(row['transcription']) else np.NaN  \n",
    "        df.at[i, 'keywords']      = preprocessing(row['keywords']) if not pd.isnull(row['keywords']) else np.NaN  \n",
    "    return df\n",
    "\n",
    "def split_data(df):\n",
    "    shuffle = df.sample(frac = 1, random_state = 42)\n",
    "\n",
    "    train_data,  test_data = train_test_split(shuffle,    test_size = 0.30, random_state = 42)\n",
    "    train_data, valid_data = train_test_split(train_data, test_size = 0.15, random_state = 42) \n",
    "\n",
    "    train_data.to_csv('../data/train.csv', index = False)\n",
    "    valid_data.to_csv('../data/valid.csv', index = False)\n",
    "    test_data. to_csv('../data/test.csv' , index = False)\n",
    "\n",
    "    data_files = {\n",
    "        'train': '../data/train.csv',\n",
    "        'valid': '../data/valid.csv',\n",
    "        'test' : '../data/test.csv'}\n",
    "    dataset = load_dataset('csv', data_files = data_files, streaming = True)\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'leave cardiac catheterization left ventriculography coronary angiography stent placement',\n",
       " 'medical_specialty': 'surgery',\n",
       " 'sample_name': 'cardiac cath coronary angiography',\n",
       " 'transcription': 'procedure leave cardiac catheterization left ventriculography coronary angiography stent placement indications atherosclerotic coronary artery disease patient history 55 year old male present 3 hour unstable angina past cardiac history history previous arteriosclerotic cardiovascular disease previous st elevation mi review systems creatinine value 1 3 mg dl mg dl procedure medication 1 visipaque 361 ml total dose 2 clopidogrel bisulphate plavix 225 mg po 3 promethazine phenergan 12 5 mg total dose 4 abciximab reopro 10 mg iv bolus 5 abciximab reopro 0 125 mcg kg minute 4 5 ml 250 ml d5w 17 ml 6 nitroglycerin 300 mcg ic total dose description procedure approach leave heart catheterization right femoral artery approach access method percutaneous needle puncture devices 1 balloon catheter utilize manufacturer boston sci quantum maverick rx 2 75 mm 20 mm 2 cordis vista brite tip 6fr jr 4 0 3 acs guidant sport 014 190 cm wire 4 stent utilize boston sci taxus rx stent 3 0 mm x 32 mm findings interventions left ventriculography overall left ventricular systolic function mildly reduce leave ventricular ejection fraction 40 left ventriculogram mild hypokinesis anterior wall left ventricle transaortic gradient mitral valve regurgitation see left main coronary artery obstructing lesion left main coronary artery blood flow appear normal left anterior descending artery 95 discrete stenosis mid left anterior descend artery drug elute boston sci taxus rx stent 3 0 mm 32 mm stent place mid left anterior descend artery post dilate 3 5 mm post procedure stenosis 0 dissection perforation left circumflex artery 50 diffuse stenosis left circumflex artery right coronary artery right coronary artery dominant posterior circulation obstructing lesion right coronary artery blood flow appear normal complication complication procedure impression 1 severe vessel coronary artery disease 2 severe leave anterior descend coronary artery disease 95 mid left anterior descend artery stenosis lesion successfully stente 3 moderate left circumflex artery disease 50 leave circumflex artery stenosis intervention warrant 4 overall left ventricular systolic function mildly reduce ejection fraction 40 mild hypokinesis anterior wall left ventricle recommendation 1 clopidogrel plavix 75 mg po daily 1 year',\n",
       " 'keywords': None}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_preprocessing()\n",
    "dataset = split_data(df)\n",
    "next(iter(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer(df, use_special):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    input_ids, attention_masks = [], []\n",
    "\n",
    "    if use_special:\n",
    "        for index, row in df.iterrows():\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                row['description'],\n",
    "                row['medical_specialty'],\n",
    "                row['sample_name'],\n",
    "                row['transcription'],\n",
    "                row['keywords'],\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "\n",
    "    else:\n",
    "        for description in df['description']:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                description,\n",
    "                add_special_tokens = True, \n",
    "                max_length = 512, \n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'leave cardiac catheterization left ventriculography coronary angiography stent placement',\n",
       " 'medical_specialty': 'surgery',\n",
       " 'sample_name': 'cardiac cath coronary angiography',\n",
       " 'transcription': 'procedure leave cardiac catheterization left ventriculography coronary angiography stent placement indications atherosclerotic coronary artery disease patient history 55 year old male present 3 hour unstable angina past cardiac history history previous arteriosclerotic cardiovascular disease previous st elevation mi review systems creatinine value 1 3 mg dl mg dl procedure medication 1 visipaque 361 ml total dose 2 clopidogrel bisulphate plavix 225 mg po 3 promethazine phenergan 12 5 mg total dose 4 abciximab reopro 10 mg iv bolus 5 abciximab reopro 0 125 mcg kg minute 4 5 ml 250 ml d5w 17 ml 6 nitroglycerin 300 mcg ic total dose description procedure approach leave heart catheterization right femoral artery approach access method percutaneous needle puncture devices 1 balloon catheter utilize manufacturer boston sci quantum maverick rx 2 75 mm 20 mm 2 cordis vista brite tip 6fr jr 4 0 3 acs guidant sport 014 190 cm wire 4 stent utilize boston sci taxus rx stent 3 0 mm x 32 mm findings interventions left ventriculography overall left ventricular systolic function mildly reduce leave ventricular ejection fraction 40 left ventriculogram mild hypokinesis anterior wall left ventricle transaortic gradient mitral valve regurgitation see left main coronary artery obstructing lesion left main coronary artery blood flow appear normal left anterior descending artery 95 discrete stenosis mid left anterior descend artery drug elute boston sci taxus rx stent 3 0 mm 32 mm stent place mid left anterior descend artery post dilate 3 5 mm post procedure stenosis 0 dissection perforation left circumflex artery 50 diffuse stenosis left circumflex artery right coronary artery right coronary artery dominant posterior circulation obstructing lesion right coronary artery blood flow appear normal complication complication procedure impression 1 severe vessel coronary artery disease 2 severe leave anterior descend coronary artery disease 95 mid left anterior descend artery stenosis lesion successfully stente 3 moderate left circumflex artery disease 50 leave circumflex artery stenosis intervention warrant 4 overall left ventricular systolic function mildly reduce ejection fraction 40 mild hypokinesis anterior wall left ventricle recommendation 1 clopidogrel plavix 75 mg po daily 1 year',\n",
       " 'keywords': None,\n",
       " 'review_length': 344}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_review_length(example):\n",
    "    return {'review_length': len(example['transcription'].split())}\n",
    "\n",
    "dataset = dataset.map(compute_review_length)\n",
    "next(iter(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_dataset = dataset.map(lambda x: tokenizer(x['transcription']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset) \n",
    "vocab.insert_token('<unk>', 0)           \n",
    "vocab.insert_token('<eos>', 1)            \n",
    "vocab.set_default_index(vocab['<unk>'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['transcription']:                                   \n",
    "            tokens = example['tokens'].append('<eos>')   \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data \n",
    "\n",
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size = 50):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.lstm = nn.LSTM(input_size  = self.bert.config.hidden_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        lstm_output, _ = self.lstm(pooled_output)\n",
    "        logits = self.fc(lstm_output[:, -1, :])  \n",
    "        return logits\n",
    "\n",
    "    \n",
    "num_classes = 47\n",
    "hidden_size = 50\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = BaselineModel(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx = 50)\n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                           hid_dim, \n",
    "                           num_layers = num_layers, \n",
    "                           bidirectional = bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, \n",
    "                                                            text_lengths.to('cpu'), \n",
    "                                                            enforce_sorted = False, \n",
    "                                                            batch_first = True)\n",
    "        \n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  \n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first = True)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)        \n",
    "        return self.fc(hn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
