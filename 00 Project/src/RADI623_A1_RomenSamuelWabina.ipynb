{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RADI623: Natural Language Processing**\n",
    "\n",
    "### Assignment: Natural Language Processing\n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University\n",
    "\n",
    "Note: In case of Python Markdown errors, you may access the assignment through this GitHub [Link](https://github.com/rrwabina/RADI605/tree/main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Medical Specialty Identification**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting oneâ€™s illnesses wrongly through self-diagnosis in medicine is very real. In a report by the [Telegraph](https://www.telegraph.co.uk/news/health/news/11760658/One-in-four-self-diagnose-on-the-internet-instead-of-visiting-the-doctor.html), nearly one in four self-diagnose instead of visiting the doctor. Out of those who misdiagnose, nearly half have misdiagnosed their illness wrongly [reported](https://bigthink.com/health/self-diagnosis/). While there could be multiple root causes to this problem, this could stem from a general unwillingness and inability to seek professional help.\n",
    "\n",
    "Elevent percent of the respondents surveyed, for example, could not find an appointment in time. This means that crucial time is lost during the screening phase of a medical treatment, and early diagnosis which could have resulted in illnesses treated earlier was not achieved.\n",
    "\n",
    "With the knowledge of which medical specialty area to focus on, a patient can receive targeted help much faster through consulting specialist doctors. To alleviate waiting times and predict which area of medical specialty to focus on, we can utilize natural language processing (NLP) to solve this task.\n",
    "\n",
    "Given any medical transcript or patient condition, this solution would predict the medical specialty that the patient should seek help in. Ideally, given a sufficiently comprehensive transcript (and dataset), one would be able to predict exactly which illness he is suffering from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import spacy\n",
    "import re\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/mtsamples.csv')\n",
    "\n",
    "num_samples = len(data)\n",
    "num_medical_specialties = data['medical_specialty'].nunique()\n",
    "\n",
    "def calculate_univariate(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    avg_description_length = description_lengths.mean()\n",
    "    min_description_length = description_lengths.min()\n",
    "    max_description_length = description_lengths.max()\n",
    "\n",
    "    avg_transcription_length = transcription_lengths.mean()\n",
    "    min_transcription_length = transcription_lengths.min()\n",
    "    max_transcription_length = transcription_lengths.max()\n",
    "\n",
    "    dictionary = {}\n",
    "    dictionary['description']   = [avg_description_length, min_description_length, max_description_length]\n",
    "    dictionary['transcription'] = [avg_transcription_length, min_transcription_length, max_transcription_length]\n",
    "    return dictionary\n",
    "summary = calculate_univariate(data)\n",
    "\n",
    "def plot_classes(data):\n",
    "    specialty_counts = data['medical_specialty'].value_counts()\n",
    "\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    plt.bar(specialty_counts.index, specialty_counts.values)\n",
    "    plt.xlabel('Medical Specialty')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Medical Specialties')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].hist(description_lengths, bins=50, alpha=0.8)\n",
    "    axs[0].set_xlabel('Description Length')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].set_title('Histogram of Description Lengths')\n",
    "\n",
    "    axs[1].hist(transcription_lengths, bins=50, alpha=0.8)\n",
    "    axs[1].set_xlabel('Transcription Length')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].set_title('Histogram of Transcription Lengths')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks(sentence):\n",
    "    sentence = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"', \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    sentence = remove_hyperlinks(sentence)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and \\\n",
    "            token.pos_ != 'SYM' and \\\n",
    "            token.pos_ != 'PUNCT' and token.pos_ != 'SPACE':\n",
    "            cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], \n",
    "                                 truncation = True, \n",
    "                                 is_split_into_words = True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def to_tokens(tokenizer, sentence):\n",
    "    inputs = tokenizer(sentence)\n",
    "    return tokenizer.convert_ids_to_tokens(inputs.input_ids)\n",
    "\n",
    "def load_preprocessing(path = '../data/mtsamples.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    for i, row in df.iterrows():\n",
    "        df.at[i, 'description']   = preprocessing(row['description'])\n",
    "        df.at[i, 'medical_specialty'] = preprocessing(row['medical_specialty'])\n",
    "        df.at[i, 'sample_name']   = preprocessing(row['sample_name'])\n",
    "        df.at[i, 'transcription'] = preprocessing(row['transcription']) if not pd.isnull(row['transcription']) else np.NaN  \n",
    "        df.at[i, 'keywords']      = preprocessing(row['keywords']) if not pd.isnull(row['keywords']) else np.NaN  \n",
    "    return df\n",
    "\n",
    "def split_data(df):\n",
    "    shuffle = df.sample(frac = 1, random_state = 42)\n",
    "\n",
    "    train_data,  test_data = train_test_split(shuffle,    test_size = 0.30, random_state = 42)\n",
    "    train_data, valid_data = train_test_split(train_data, test_size = 0.15, random_state = 42) \n",
    "\n",
    "    train_data.to_csv('../data/train.csv', index = False)\n",
    "    valid_data.to_csv('../data/valid.csv', index = False)\n",
    "    test_data. to_csv('../data/test.csv' , index = False)\n",
    "\n",
    "    data_files = {\n",
    "        'train': '../data/train.csv',\n",
    "        'valid': '../data/valid.csv',\n",
    "        'test' : '../data/test.csv'}\n",
    "    dataset = load_dataset('csv', data_files = data_files, delimiter = '\\t', streaming = True)\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description,medical_specialty,sample_name,transcription,keywords': ' Care conference with family at the bedside and decision to change posture of care from aggressive full code status to terminal wean with comfort care measures in a patient with code last night with CPR and advanced cardiac life support., Hospice - Palliative Care, Care Conference With Family ,\"REASON FOR FOLLOWUP:,  Care conference with family at the bedside and decision to change posture of care from aggressive full code status to terminal wean with comfort care measures in a patient with code last night with CPR and advanced cardiac life support.,HISTORY OF PRESENT ILLNESS: , This is a 65-year-old patient originally admitted by me several weeks ago with profound hyponatremia and mental status changes.  Her history is also significant for likely recurrent aspiration pneumonia and intubation earlier on this admission as well.  Previously while treating this patient I had met with the family and discussed how aggressive the patient would wish her level of care to be given that there was evidence of possible ovarian malignancy with elevated CA-125 and a complex mass located in the ovary.  As the patient was showing signs of improvement with some speech and ability to follow commands, decision was made to continue to pursue an aggressive level of care, treat her dysphagia, hypertension, debilitation and this was being done.  However, last night the patient had apparently catastrophic event around 2:40 in the morning.  Rapid response was called and the patient was intubated, started on pressure support, and given CPR.  This morning I was called to the bedside by nursing stating the family had wished at this point not to continue this aggressive level of care.  The patient was seen and examined, she was intubated and sedated.  Limbs were cool.  Cardiovascular exam revealed tachycardia.  Lungs had coarse breath sounds.  Abdomen was soft.  Extremities were cool to the touch.  Pupils were 6 to 2 mm, doll\\'s eyes were not intact.  They were not responsive to light.  Based on discussion with all family members involved including both sons, daughter and daughter-in-law, a decision was made to proceed with terminal wean and comfort care measures.  All pressure support was discontinued.  The patient was started on intravenous morphine and respiratory was requested to remove the ET tube.  Monitors were turned off and the patient was made as comfortable as possible.  Family is at the bedside at this time.  The patient appears comfortable and the family is in agreement that this would be her wishes per my understanding of the family and the patient dynamics over the past month, this is a very reasonable and appropriate approach given the patient\\'s failure to turn around after over a month of aggressive treatment with likely terminal illness from ovarian cancer and associated comorbidities.,Total time spent at the bedside today in critical care services, medical decision making and explaining options to the family and proceeding with terminal weaning was excess of 37 minutes.\",\"hospice - palliative care, full code status, terminal wean, comfort care, cpr, advanced cardiac life support, care conference, family, bedsideNOTE,: Thesetranscribed medical transcription sample reports and examples are provided by various users andare for reference purpose only. MTHelpLine does not certify accuracy and quality of sample reports.These transcribed medical transcription sample reports may include some uncommon or unusual formats;this would be due to the preference of the dictating physician. All names and dates have beenchanged (or removed) to keep confidentiality. Any resemblance of any type of name or date orplace or anything else to real world is purely incidental.\"'}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_dataset = df.map(lambda x: tokenizer(x['transciption']))\n",
    "shuffled_dataset  = df.shuffle(buffer_size = 10_000, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description,medical_specialty,sample_name,transcription,keywords': ' Left Cardiac Catheterization, Left Ventriculography, Coronary Angiography and Stent Placement., Surgery, Cardiac Cath & Coronary Angiography ,\"PROCEDURE:,  Left Cardiac Catheterization, Left Ventriculography, Coronary Angiography and Stent Placement.,INDICATIONS: , Atherosclerotic coronary artery disease.,PATIENT HISTORY: , This is a 55-year-old male.  He presented with 3 hours of unstable angina.,PAST CARDIAC HISTORY: , History of previous arteriosclerotic cardiovascular disease.  Previous ST elevation MI.,REVIEW OF SYSTEMS.,  The creatinine value is 1.3 mg/dL mg/dL.,PROCEDURE MEDICATIONS:,1.  Visipaque 361 mL total dose.,2.  Clopidogrel bisulphate (Plavix) 225 mg PO,3.  Promethazine (Phenergan) 12.5 mg total dose.,4.  Abciximab (Reopro) 10 mg IV bolus,5.  Abciximab (Reopro) 0.125 mcg/kg/minute, 4.5 mL/250 mL D5W x 17 mL,6.  Nitroglycerin 300 mcg IC total dose.,DESCRIPTION OF PROCEDURE:,APPROACH: , Left heart catheterization via right femoral artery approach.,ACCESS METHOD: , Percutaneous needle puncture.,DEVICES USED:,1.  Balloon catheter utilized:  Manufacturer:  Boston Sci Quantum Maverick RX 2.75mm x 20mm.,2.  Cordis Vista Brite Tip 6Fr JR 4.0,3.  ACS/Guidant Sport .014\"\" (190cm) Wire,4.  Stent utilized:  Boston Sci Taxus RX Stent 3.0mm x 32mm.,FINDINGS/INTERVENTIONS:,LEFT VENTRICULOGRAPHY:,  The overall left ventricular systolic function is mildly reduced.  Left ventricular ejection fraction is 40% by left ventriculogram.  Mild hypokinesis of the anterior wall of the left ventricle.  There was no transaortic gradient.  Mitral valve regurgitation is not seen.,LEFT MAIN CORONARY ARTERY:  ,  There were no obstructing lesions in the left main coronary artery.  Blood flow appeared normal.,LEFT ANTERIOR DESCENDING ARTERY: , There was a 95%, discrete stenosis in the mid left anterior descending artery.  A drug eluting, Boston Sci Taxus RX Stent 3.0mm x 32mm stent was placed in the mid left anterior descending artery and post-dilated to 3.5 mm.  Post-procedure stenosis was 0%.  There was no dissection and no perforation.,LEFT CIRCUMFLEX ARTERY: , There was a 50%, diffuse stenosis in the left circumflex artery.,RIGHT CORONARY ARTERY:,  The right coronary artery is dominant to the posterior circulation.  There were no obstructing lesions in the right coronary artery.  Blood flow appeared normal.,COMPLICATIONS:,There were no complications during the procedure., ,IMPRESSION:,1.  Severe two-vessel coronary artery disease.,2.  Severe left anterior descending coronary artery disease.  There was a 95% mid left anterior descending artery stenosis.  The lesion was successfully stented.,3.  Moderate left circumflex artery disease.  There was a 50% left circumflex artery stenosis.  Intervention not warranted.,4.  The overall left ventricular systolic function is mildly reduced with ejection fraction of 40%.  Mild hypokinesis of the anterior wall of the left ventricle.,RECOMMENDATION:,1.  Clopidogrel (Plavix) 75 mg PO daily for 1 year.\",'},\n",
       " {'description,medical_specialty,sample_name,transcription,keywords': ' MRI Brain and Brainstem - Falling (Multiple System Atrophy), Radiology, MRI Brain and Brainstem ,\"CC:, Falling.,HX:, This 67y/o RHF was diagnosed with Parkinson\\'s Disease in 9/1/95, by a local physician. For one year prior to the diagnosis, the patient experienced staggering gait, falls and episodes of lightheadedness. She also noticed that she was slowly \"\"losing\"\" her voice, and that her handwriting was becoming smaller and smaller. Two months prior to diagnosis, she began experienced bradykinesia, but denied any tremor. She noted no improvement on Sinemet, which was started in 9/95. At the time of presentation, 2/13/96, she continued to have problems with coordination and staggering gait. She felt weak in the morning and worse as the day progressed. She denied any fever, chills, nausea, vomiting, HA, change in vision, seizures or stroke like events, or problems with upper extremity coordination.,MEDS:, Sinemet CR 25/100 1tab TID, Lopressor 25mg qhs, Vitamin E 1tab TID, Premarin 1.25mg qd, Synthroid 0.75mg qd, Oxybutynin 2.5mg has, isocyamine 0.125mg qd.,PMH:, 1) Hysterectomy 1965. 2) Appendectomy 1950\\'s. 3) Left CTR 1975 and Right CTR 1978. 4) Right oophorectomy 1949 for \"\"tumor.\"\" 5) Bladder repair 1980 for unknown reason. 6) Hypothyroidism dx 4/94. 7) HTN since 1973.,FHX: ,Father died of MI, age 80. Mother died of MI, age73. Brother died of Brain tumor, age 9.,SHX: ,Retired employee of Champion Automotive Co.,Denies use of TOB/ETOH/Illicit drugs.,EXAM: ,BP (supine)182/113 HR (supine)94. BP (standing)161/91 HR (standing)79. RR16 36.4C.,MS: A&O to person, place and time. Speech fluent and without dysarthria. No comment regarding hypophonia.,CN: Pupils 5/5 decreasing to 2/2 on exposure to light. Disks flat. Remainder of CN exam unremarkable.,Motor: 5/5 strength throughout. NO tremor noted at rest or elicited upon movement or distraction,Sensory: Unremarkable PP/VIB testing.,Coord: Did not show sign of dysmetria, dyssynergia, or dysdiadochokinesia. There was mild decrement on finger tapping and clasping/unclasping hands (right worse than left).,Gait: Slow gait with difficulty turning on point. Difficulty initiating gait. There was reduced BUE swing on walking (right worse than left).,Station: 3-4step retropulsion.,Reflexes: 2/2 and symmetric throughout BUE and patellae. 1/1 Achilles. Plantar responses were flexor.,Gen Exam: Inremarkable. HEENT: unremarkable.,COURSE:, The patient continued Sinemet CR 25/100 1tab TID and was told to monitor orthostatic BP at home. The evaluating Neurologist became concerned that she may have Parkinsonism plus dysautonomia.,She was seen again on 5/28/96 and reported no improvement in her condition. In addition she complained of worsening lightheadedness upon standing and had an episode, 1 week prior to 5/28/96, in which she was at her kitchen table and became unable to move. There were no involuntary movements or alteration in sensorium/mental status. During the episode she recalled wanting to turn, but could not. Two weeks prior to 5/28/96 she had an episode of orthostatic syncope in which she struck her head during a fall. She discontinued Sinemet 5 days prior to 5/28/96 and felt better. She felt she was moving slower and that her micrographia had worsened. She had had recent difficulty rolling over in bed and has occasional falls when turning. She denied hypophonia, dysphagia or diplopia.,On EXAM: BP (supine)153/110 with HR 88. BP (standing)110/80 with HR 96. (+) Myerson\\'s sign and mild hypomimia, but no hypophonia. There was normal blinking and EOM. Motor strength was full throughout. No resting tremor, but mild postural tremor present. No rigidity noted. Mild decrement on finger tapping noted. Reflexes were symmetric. No Babinski signs and no clonus. Gait was short stepped with mild anteroflexed posture. She was unable to turn on point. 3-4 step Retropulsion noted. The Parkinsonism had been unresponsive to Sinemet and she had autonomic dysfunction suggestive of Shy-Drager syndrome. It was recommended that she liberalize dietary salt use and lie with head of the bed elevate at 20-30 degrees at night. Indomethacin was suggested to improve BP in future.\",\"radiology, myerson\\'s sign, falling, dysautonomia, mri brain and brainstem, brain and brainstem, mri brain, sinemet cr, mri, brainstem, ctr, tumor, retropulsion, parkinsonism, brain, lightheadedness, hypophonia, standing, sinemet, \"'}]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_head = dataset['train'].take(2)\n",
    "list(dataset_head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>preprocessing</code> function takes a sentence, removes hyperlinks, performs various token-level filters (removing stop words, symbols, punctuation marks, and whitespace), lemmatizes the remaining tokens to their base forms, and returns the cleaned sentence as a string. Specifically, the code <code>token.pos_ != 'SYM' and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE'</code> checks if the token's part-of-speech (POS) tag is not 'SYM' (symbol), 'PUNCT' (punctuation), or 'SPACE'. It further filters out tokens that are symbols, punctuation marks, or represent whitespace. We also appended the lowercase lemma (base form) of the token, obtained using <code>token.lemma_</code>, to the cleaned_tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer(df, use_special):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    input_ids, attention_masks = [], []\n",
    "\n",
    "    if use_special:\n",
    "        for index, row in df.iterrows():\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                row['description'],\n",
    "                row['medical_specialty'],\n",
    "                row['sample_name'],\n",
    "                row['transcription'],\n",
    "                row['keywords'],\n",
    "                max_length = 512,\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "\n",
    "    else:\n",
    "        for description in df['description']:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                description,\n",
    "                add_special_tokens = True, \n",
    "                max_length = 512, \n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size = 50):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.lstm = nn.LSTM(input_size  = self.bert.config.hidden_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        lstm_output, _ = self.lstm(pooled_output)\n",
    "        logits = self.fc(lstm_output[:, -1, :])  \n",
    "        return logits\n",
    "\n",
    "    \n",
    "num_classes = 47\n",
    "hidden_size = 50\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = BaselineModel(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx = 50)\n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                           hid_dim, \n",
    "                           num_layers = num_layers, \n",
    "                           bidirectional = bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)        \n",
    "        return self.fc(hn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
